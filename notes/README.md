---
title: Notes on minimization problems using pytorch
author: Debdeep Bhattacharya
include-header: |

    \newcommand{\1}{\mathcal{1}}
    \newcommand{\F}{\mathcal{F}}
    \newcommand{\E}{\mathcal{E}}
    \newcommand{\R}{\mathbb{R}}
    \newcommand{\C}{\mathbb{C}}
    \newcommand{\N}{\mathbb{N}}
    \newcommand{\Z}{\mathbb{Z}}

    \newcommand{\ww}{\boldsymbol{\omega}}
    \newcommand{\sigmaB}{\boldsymbol{\sigma}}
    \newcommand{\1}{\boldsymbol{1}}
    \newcommand{\aa}{\mathbf{a}}
    \newcommand{\bb}{\mathbf{b}}
    \newcommand{\ee}{\mathbf{e}}
    \newcommand{\ff}{\mathbf{f}}
    \newcommand{\FF}{\mathbf{F}}
    \newcommand{\hh}{\mathbf{h}}
    \newcommand{\uu}{\mathbf{u}}
    \newcommand{\vv}{\mathbf{v}}
    \newcommand{\xx}{\mathbf{x}}
    \newcommand{\yy}{\mathbf{y}}
    \newcommand{\zz}{\mathbf{z}}

    \newcommand{\what}{\bb{??}}
    \newcommand{\half}{\frac{1}{2}}
    \newcommand{\norm}[1]{\left\lVert#1\right\rVert}
    \newcommand{\abs}[1]{\left\lvert#1\right\rvert}
    \newcommand{\jap}[1]{\left\langle #1 \right\rangle}
    \newcommand{\inn}[1]{\left\langle #1 \right\rangle}
---

We will enhance the nice [pytorch example](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html) with extra explanations.

# Problem setup



The goal is the fit $\sin x$ using a cubic polynomial.

The data will be generated by sampling the curve $y = \sin x$. But this relationship between $x$ and $y$ will not be known to the modeler.

Let the data $\{(x_i, y_i)\}_{i=1}^n$ be given.

Our model is $y = f(x)$ where 
\begin{align}
    \label{eq:model-cubic}
f_{\ww}(x) = a + bx + cx^2 + d x^3,
\end{align}
where $\ww = (a, b, c, d)$ are the free parameters to minimizer over.

The minimization problem is therefore  

\begin{align}
    \label{eq:problem-minimization}
\min \left\{\sum_{i=1}^{n} \abs{y_i - f_{\ww}(x_i)}^2: \ww \in \R^4 \right\}
\end{align}

To minimize, we use gradient descent method on $\eqref{eq:problem-minimization}$.  Defining the *loss function* (objective function)
$$
L(\ww) = \sum_{i=1}^{n} \abs{f_{\ww}(x_i) - y_i}^2
$$

and then use the scheme
$$
\ww_{n+1} = \ww_n - \eta \nabla_{\ww} L(\ww_n)
$$
with initial guess $\ww_0$ and *learning rate* (numerical step size) $\eta$.

We would need to compute
$$
\nabla_{\ww} L(\ww) = 2 \sum_{i=1}^{n} (f_{\ww}(x_i) - y_i) \nabla_{\ww} f_{\ww}(x_i)
$$
explicitly.
For our model $f_{\ww}$, noting that
\begin{align*}
    \nabla_{\ww} f_{\ww}(x_i) = \begin{bmatrix} 1 \\ x_i \\ x_i^2 \\ x_i^3 \end{bmatrix} 
\end{align*}
we have
\begin{align*}
    \frac{\partial L}{\partial a}(\ww) & = 2 \sum_{i=1}^{n} (f_{\ww}(x_i) - y_i)
    \\
    \frac{\partial L}{\partial b}(\ww) &= 2 \sum_{i=1}^{n} (f_{\ww}(x_i) - y_i) x_i
    \\
    \frac{\partial L}{\partial c}(\ww) &= 2 \sum_{i=1}^{n} (f_{\ww}(x_i) - y_i) x_i^2
    \\
    \frac{\partial L}{\partial d}(\ww) &= 2 \sum_{i=1}^{n} ( f_{\ww}(x_i) - y_i) x_i^3
\end{align*}

In summary,
\begin{align*}
    \nabla_{\ww} L(\ww_n) = \begin{bmatrix} \uu \cdot \1 \\ \uu \cdot \xx \\ \uu \cdot \xx^2 \\ \uu \cdot \xx^3 \end{bmatrix} 
\end{align*}
where $\1 \in \R^n$ is a vector of ones, $\xx^n \in \R^n$ is elementwise $n$-th power of $\xx = (x_i)_{i=1}^n$, and $\uu = 2 (f_{\ww_n}(x_i) - y_i)_{i=1}^n$.

This is done in the following python code:

```python
import numpy as np
import math

# Create random input and output data
x = np.linspace(-math.pi, math.pi, 2000)
y = np.sin(x)

# Randomly initialize weights
a = np.random.randn()
b = np.random.randn()
c = np.random.randn()
d = np.random.randn()

learning_rate = 1e-6
for t in range(2000):
    # Forward pass: compute predicted y
    # y = a + b x + c x^2 + d x^3
    y_pred = a + b * x + c * x ** 2 + d * x ** 3

    # Compute and print loss
    loss = np.square(y_pred - y).sum()
    if t % 100 == 99:
        print(t, loss)

    # Backprop to compute gradients of a, b, c, d with respect to loss
    grad_y_pred = 2.0 * (y_pred - y)
    grad_a = grad_y_pred.sum()
    grad_b = (grad_y_pred * x).sum()
    grad_c = (grad_y_pred * x ** 2).sum()
    grad_d = (grad_y_pred * x ** 3).sum()

    # Update weights
    a -= learning_rate * grad_a
    b -= learning_rate * grad_b
    c -= learning_rate * grad_c
    d -= learning_rate * grad_d

print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')
```

# Automatic differentiation

The minimization problems are set up using the loss function, which is a **composition** of several functions $g^1: \R^4 \to \R^n$, $g^2: \R^n \to \R^n$, and $g^3: \R^n \to \R$

\begin{align*}
    g^1(\ww) &= (y_i - f_{\ww}(x_i))_{i=1}^n
\\
    g^2(\xx) &= (x_i^2)_{i=1}^n
\\
    g^3(\xx) &= \sum_{i=1}^{n} x_i
\end{align*}
Then,
$$
L(\ww) = g^3(g^2(g^1(\ww)))
$$
Therefore, the $k$-th partial derivative of the loss function is

\begin{align*}
\frac{\partial}{\partial \omega_k} L(\ww) 
    & = \sum_{i=1}^{n} \frac{\partial }{\partial x_i} g^3(\xx)|_{\xx = g^2(g^1(\ww))} \frac{\partial}{\partial \omega_k}  g^2_i(g^1(\ww))
\\
    & = \sum_{i=1}^{n} \frac{\partial }{\partial x_i} g^3(\xx)|_{\xx = g^2(g^1(\ww))} \sum_{j=1}^{n} \frac{\partial}{\partial x_j}  g^2_i(\xx)|_{\xx = g^1(\ww)} \frac{\partial}{\partial \omega_k}  g^1_j(\ww)
\end{align*}

More generally, if $\ww \in \R^p$ (i.e., $p$ parameters to minimize), 
and if $L$ is a composition of $M$ functions
$$
L(\ww) = (g^{M} \circ g^{M-1} \circ \dots \circ g^1)(\ww),
$$
where $g^i : \R^{p_{i-1}} \to \R^{p_i}$ for all $i=1, \dots, M$.

For dimensional consistency, we must have $p_0 =0$ and $p_M = 1$. Therefore, we have
$g^1: \R^p \to \R^{p_1}$, $g^2: \R^{p_1} \to \R^{p_2}, \dots,  g^{M-1}: \R^{p_{M-2}} \to \R^{p_{M-1}}$, $g^M: \R^{p_{M-1}} \to \R$.

Differentiating with respect to the parameter $w_k$, 
we can write
\begin{align*}
    \frac{\partial}{\partial \omega_k} L(\ww)  
    = &  
    \sum_{i_{M-1}=1}^{p_{M-1}} 
    \frac{\partial }{\partial x_{i_{M-1}}} g^M(\xx^{M-1})
    \sum_{i_{M-2}=1}^{p_{M-2}} 
    \frac{\partial }{\partial x_{i_{M-2}}} g_{i_{M-1}}^{M-1}(\xx^{M-2})
    \dots
    \\
    & 
    \sum_{i_{M_1}=1}^{p_1} 
    \frac{\partial }{\partial x_{i_{M_1}}} g_{i_{M_2}}^{2}(\xx^1)
    % \sum_{i_{M_1}=1}^{p_1} 
    \frac{\partial }{\partial \omega_{k}} g_{i_{M_1}}^{1}(\ww)
    \\
    = &  
    \sum_{i_{M-1}=1}^{p_{M-1}} 
    \sum_{i_{M-2}=1}^{p_{M-2}} 
    \dots
    \sum_{i_{M_1}=1}^{p_1} 
    \frac{\partial }{\partial x_{i_{M-1}}} g^M(\xx^{M-1})
    \frac{\partial }{\partial x_{i_{M-2}}} g_{i_{M-1}}^{M-1}(\xx^{M-2})
    \dots
    \\
    & 
    \frac{\partial }{\partial x_{i_{M_1}}} g_{i_{M_2}}^{2}(\xx^1)
    % \sum_{i_{M_1}=1}^{p_1} 
    \frac{\partial }{\partial \omega_{k}} g_{i_{M_1}}^{1}(\ww)
    \\
\end{align*}
where $\xx^i$ is defined as $(g^i \circ g^{i-1} \circ \dots \circ g^1) (\ww)$ for each $i=1, 2, \dots, M-1$.

Therefore, to compute $\nabla_{\ww} L(\ww)$, one needs to know the tensor
$$
d_{rst} = \frac{\partial}{\partial x_r} g^s_t(\xx^{s-1})
$$
for all $s=1, \dots, M$, $t=1, \dots, p_s$, and $r=1, \dots, p_{s-1}$.
Combining all $d_{rst}$ to compute $\nabla_{\ww} L(\ww)$ is know as *backward propagation*.


Note that the tensor $d_{rst}$ describes the $r$-th partial derivative of $t$-th component of the function $g^s$, evaluated at the immediate value $\xx^{s-1}$.

For notational convenience, one could use Einstein's convention for summation (repeated entries are summed) and simply write
\begin{align*}
\frac{\partial}{\partial w_k}L(\ww)
    = 
    \prod_{s=2}^{M} \prod_{t=1}^{p_s} \prod_{r=1}^{p_{s-1}} 
    \frac{\partial}{\partial x_r} g^s_t(\xx^{s-1}) 
    \frac{\partial}{\partial w_k} g^1_{i_{M_1}}(\ww)
\end{align*}
but we do not adapt this notation here.


The graph-theoretic realization of this procedure is a directed graph from left to right with leaves on the left as $\omega_k$. Edges of the graph represent the partial derivates of the target nodes with respect to the source nodes evaluated at the value of the source node. The last node on the right is the loss function $L$
In this setup, back-propagation procedure populates the leaf nodes $\omega_k$ with $\frac{\partial}{\partial \omega_k} L(\ww)$.


## Auto-differentiation in pytorch

Let $\ww = (a, b, c, d) \in \R^4$.

In `pytorch`, setting `requires_grad=True` while defining a variable $a$ implies we would want to compute $\frac{\partial}{\partial a}$ of a function of $a$ at some point.

```python
# Setting requires_grad=True indicates that we want to compute gradients with
# respect to these Tensors during the backward pass.
a = torch.randn((), dtype=dtype, requires_grad=True)
b = torch.randn((), dtype=dtype, requires_grad=True)
c = torch.randn((), dtype=dtype, requires_grad=True)
d = torch.randn((), dtype=dtype, requires_grad=True)
```

Given (fixed) data `x` and `y`, we define a loss function $L(\ww)$ called `loss`

```python
y_pred = a + b * x + c * x ** 2 + d * x ** 3
loss = (y_pred - y).pow(2).sum()
```

**Remark:** Any variable such as `y_pred` and `loss` defined as a function of variables with `requires_grad=True` (such as `a, b, c, d`), will automatically have `requires_grad=True`. This feature makes sure that a computational graph gets created to store the subsequent partial derivatives $d_{rst}$. To define *inferential* variables (variables you do not plan to compute partial derivative of) you need to turn this off manually using `torch.no_grad()` like this

```
>>> with torch.no_grad():
...     y = x * 2
>>> y.requires_grad
False

>>> @torch.no_grad()
... def tripler(x):
...     return x * 3
>>> z = tripler(x)
>>> z.requires_grad
```

At some point during our computation, we will need to compute the partial derivative 
$\frac{\partial}{\partial a}L(\ww^n)$ using the current value of $\ww = \ww^n$. In fact, we can compute all the partial derivatives $\nabla_{\ww} L(\ww^n)$ at once by calling the `backward()` function on the objective function $L$ (`loss`) like this:

```python
# Use autograd to compute the backward pass. This call will compute the
# gradient of loss with respect to all Tensors with requires_grad=True.
# After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding
# the gradient of the loss with respect to a, b, c, d respectively.
loss.backward()
```

At this point, all partial derivates of `loss` function $L$ with respect to all variables with a `requires_grad=True` is computed at the current value of $\ww = \ww^n$. We can get the value of $\frac{\partial}{\partial a}L(\ww^n)$ at the current value $\ww = \ww^n$ using `a.grad`.

```python
a.grad
b.grad
c.grad
d.grad
```

### Updating the parameters (variables with `requires_grad=True`)

$$\frac{\partial}{\partial a}L(\ww^n)
\to \frac{\partial}{\partial a}L(\ww^{n+1})$$

Note that we would want to update the value of $\ww$ (in particular, the value of $a$) from $\ww^n$ to $\ww^{n+1}$, for example, during a gradient descent method. This should change $\frac{\partial}{\partial a}L(\ww^n)$, but the update does not happen unless you run `loss.backward()` again. 

**Remark:** While manually updating variables with `requires_grad=True`, we turn off gradient computation (why?). We would recompute the gradient again anyway, and do not want to spend computational power computing the gradients of update functions. Therefore, do the following: 

```python
# Manually update weights using gradient descent. Wrap in torch.no_grad()
# because weights have requires_grad=True, but we don't need to track this
# in autograd.
with torch.no_grad():
    a -= learning_rate * a.grad
    b -= learning_rate * b.grad
    c -= learning_rate * c.grad
    d -= learning_rate * d.grad

    # Manually zero the gradients after updating weights
    a.grad = None
    b.grad = None
    c.grad = None
    d.grad = None
```


## Neural network approximation

Consider the function $f_d$ representing the output of a convolutional neural network of depth $d \ge 1$ with *activation function* $\sigma$ defined recursively by
\begin{align*}
    f_1(x) &= \sigma(a_1 x + b_1)
    \\
    f_2(x) &= \sigma(a_2 f_1(x) + b_2)
    \\
    \vdots
    \\
    f_d(x) &= \sigma(a_d f_{d-1}(x) + b_d).
\end{align*}

Few common choices of $\sigma(x)$ are $x$, $\tanh(x)$, Heaviside, and logistic function. The neural network model is therefore
\begin{align*}
    y = f_d(x)
\end{align*}
where $a_1, \dots, a_d$ and $b_1, \dots, b_d$ are the parameters of the model. The corresponding $l^2$ minimization problem is
\begin{align*}
    \min_{a_1, \dots, a_d, b_1, \dots, b_d \in \R} \sum_{i=1}^{n} \abs{y_i - f_d(x_i)}^2.
\end{align*}

There is no general closed-form solution to the minimization problem. Methods like gradient descent can be use to find a minimizer numerically.
Consider the function $f_d$ representing the output of a convolutional neural network of depth $d \ge 1$ with *activation function* $\sigma$ defined recursively by
\begin{align*}
    f_1(x) &= \sigma(a_1 x + b_1)
    \\
    f_2(x) &= \sigma(a_2 f_1(x) + b_2)
    \\
    \vdots
    \\
    f_d(x) &= \sigma(a_d f_{d-1}(x) + b_d).
\end{align*}
Few common choices of $\sigma(x)$ are $x$, $\tanh(x)$, Heaviside, and logistic function. The neural network model is therefore
\begin{align*}
    y = f_d(x)
\end{align*}	
where $a_1, \dots, a_d$ and $b_1, \dots, b_d$ are the parameters of the model. The corresponding $l^2$ minimization problem is
\begin{align*}
    \min_{a_1, \dots, a_d, b_1, \dots, b_d \in \R} \sum_{i=1}^{n} \abs{y_i - f_d(x_i)}^2.
\end{align*}	

There is no general closed-form solution to the minimization problem. Methods like gradient descent can be use to find a minimizer numerically.

### Higher-dimensional formulation

Let the $i$-th data point be $(\xx_i, \yy_i)$, where $\xx_i \in \R^l$ (i.e. data has $p$ *features* and $q$ labels). 

An affine function $g_{W, \bb}: \R^p \to \R^s$ is defined by
\begin{align*}
    g_{W, \bb}(\xx) = W\xx + \bb,
\end{align*}	
where $W \in \R^{s\times p}$ is called the *weight* and $\bb \in \R^s$ is called the *bias*.

An *activation function* $\sigma: \R \to \R$ is a differentiable, nonlinear function. Acting element-wise, with a slight abuse of notation, we define $\sigma: \R^n \to \R^n$ by
\begin{align*}
    \sigma(\xx) = (\sigma(x_i))_{i=1}^n.
\end{align*}	

A *single linear neural layer* $f:\R^p \to \R^s$ with weight $W \in \R^{s \times p}$, bias $\bb \in \R^s$, and activation function $\sigma: \R^s \to \R^s$ is defined as
\begin{align*}
f = \sigma \circ g.
\end{align*}	

A depth-$d$ neural network *model* is therefore given by
\begin{align*}
    N^d(\xx) = (f^d \circ \dots \circ f^1) (\xx)
\end{align*}	
where  for all $i=1, \dots, d$,
\begin{align*}
    f^i = \sigma^i \circ g^i,
\end{align*}	
where $g^i$ is a linear neural network and $\sigma^i$ is an activation function.
Let for all $i=1, \dots, d$, we have $g^i: \R^{p_{i-1}} \to \R^{p_{i}}$ with weight $W^i \in \R^{p_{i} \times p_{i-1}}$ and bias $\bb^i \in \R^{p_i}$.

For dimensional consistency, we must have
$p_0 = p$  and $p_d=q$.

A loss function $L$ over $n$ data points $\{ (\xx_i, \yy_i) \}_{i=1}^n$ is a function 
\begin{align*}
    L(W^d, \dots, W^1, \bb^d, \dots, \bb^1) =  \sum_{i=1}^{n} \norm{N^d(\xx_i) - \yy_i}^2.
\end{align*}	

### Gradient of the loss function

To compute the gradient of the loss function with respect to each bias $b^i_j$ and weight $w^i_{jk}$, 
recalling
\begin{align*}
    g^i_l(\zz) = \sum_{s=1}^{p_{m-1}} W^i_{ls} z_s + b^i_l
\end{align*}	
we need the derivatives 
\begin{align*}
    \frac{d}{db^i_{j}}g^m_l(\zz) = 
    \frac{\partial}{\partial b^i_{j}}  b^m_l = 
     \delta_{mi} \delta_{lj}
\end{align*}	
and
\begin{align*}
    \frac{d}{dw^i_{jk}}g^m_l(\zz) 
    &= 
    \frac{\partial}{\partial w^i_{jk}} \left(\sum_{s=1}^{p_{m-1}} W^m_{ls} z_s + b^m_l\right) 
    \\
    &= \sum_{s=1}^{p_{m-1}} \delta_{mi} \delta_{lj}\delta_{sk} z_s
    \\
    &= \delta_{mi} \delta_{lj} z_k,
\end{align*}	
where $\delta_{kj}$ denotes the Kronecker's delta
\begin{align*}
    \delta_{kj} =
    \begin{cases}
       1, &\text{ if } k = j
       \\ 
       0, &\text{ otherwise }.
    \end{cases}
\end{align*}	

The gradient of the element-wise activation function
$\sigma^m$ is 
\begin{align*}
    \frac{\partial}{\partial z_j} \sigma^m_k(\zz) = 
    \frac{\partial}{\partial z_j} \sigma^m(z_k) = (\sigma^m)'(z_k) \delta_{kj}.
\end{align*}	

Therefore, the partial derivatives of each linear neural layer $f^m$ are
\begin{align*}
    \frac{d}{db^i_{j}}f^m_l(\zz) 
    &= 
    \frac{d}{db^i_{j}} \sigma^m_l(g^m(\zz))
    \\
    &= \sum_{k=1}^{p_{m-1}} \frac{\partial}{\partial z_k} \sigma^m_l(g^m(\zz)) \frac{\partial}{\partial b^i_j} g^m_k(\zz)
    \\
    & = 
    \sum_{k=1}^{p_{m-1}}
    (\sigma^m)'(g^m_l(\zz)) \delta_{lk} 
    \delta_{mi} \delta_{kj}
    \\
    & = 
    (\sigma^m)'(g^m_l(\zz))
    \delta_{mi} \delta_{lj}
\end{align*}	
and
\begin{align*}
    \frac{d}{dw^i_{jk}}f^m_l(\zz) 
    &= \sum_{s=1}^{p_{m-1}} \frac{\partial}{\partial z_s} \sigma^m_l(g^m(\zz)) \frac{\partial}{\partial w^i_{jk}} g^m_s(\zz)
    \\
    &=
    \sum_s
    (\sigma^m)'(g^m_l(\zz)) \delta_{ls} 
    \delta_{mi} \delta_{sj} z_k
    \\
    &= 
    (\sigma^m)'(g^m_l(\zz)) z_k
    \delta_{mi} \delta_{lj} .
\end{align*}	


### Example of a neural network in pytorch

We construct a  neural network $N^d$ of depth $d=2$ for data with $p = 4$ features and $q=1$ label(s).
We consider
\begin{align*}
g^1: \R^4 \to \R^8
\\
g^2: \R^8 \to \R^1
\end{align*}	
with $\sigma^1(r) = \max\{0, r\}$ (called ReLU($r$)) and $\sigma^2(r) = r$. The affine maps are defined as
\begin{align*}
    g^i(\zz) = W^i \zz + b^i
\end{align*}	
for each $\zz \in$ Domain($g^i$) for $i=1,2$.
Then, the depth-2 neural network $N^d$ is defined as
\begin{align*}
    N^d(\xx) = (g^2 \circ \sigma^1 \circ g^1) (\xx)
\end{align*}	

The weights and biases of each affine map are $W^1 \in \R^{8 \times 4}, W^2 \in \R^{1 \times 8}$ and $\bb^1 \in \R^8$ and $\bb^2 \in \R^1$.

For each of $\xx_i$ of the data set $\{ (\xx_i, y_i) \}_{i=1}^3$, we compute $N^d(\xx_i)$. In the following code, `X` = $(\xx_i)_{i=1}^3$ and `net(X)` = $(N^d(\xx_i))_{i=1}^3$.


```python
import torch
from torch import nn

net = nn.Sequential( 
                    nn.Linear(4,8),
                    nn.ReLU(),
                    nn.Linear(8,1)
                    )
X = torch.rand(size=(3,4))

print('X', X)
print('net(X)', net(X))

## Use net.state_dict() to show the parameters
print('Show parameters of the first neural network in the sequence:', net[0].state_dict())
print('Show parameters of the third neural network in the sequence:', net[2].state_dict())
print('print the bias data of 3rd net:', net[2].bias.data)
```

### Custom model involving neural networks

To use optimized pytorch features such as `torch.nn.MSELoss()`, `torch.optim.SGD()`, `optimizer.step()` etc for your own model, you need to define your own model as a module, which is a derived class of `torch.nn.Module`.

```python
class Polynomial3(torch.nn.Module):
    def __init__(self):
        """
        In the constructor we instantiate four parameters and assign them as
        member parameters.
        """
        super().__init__()
        self.a = torch.nn.Parameter(torch.randn(()))
        self.b = torch.nn.Parameter(torch.randn(()))
        self.c = torch.nn.Parameter(torch.randn(()))
        self.d = torch.nn.Parameter(torch.randn(()))

    def forward(self, x):
        """
        In the forward function we accept a Tensor of input data and we must return
        a Tensor of output data. We can use Modules defined in the constructor as
        well as arbitrary operators on Tensors.
        """
        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3

    def string(self):
        """
        Just like any class in Python, you can also define custom method on PyTorch modules
        """
        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'

# Construct our model by instantiating the class defined above
model = Polynomial3()
```

Note that we do not need to define a `backward()` method for a `torch.nn.Module` as it is derived from the operations specified within the `forward()` method.
If your model (`torch.nn.Module`) uses an exotic function which you define via a `torch.autograd.Function`, then you define the derivate of the function within the `backward()` method of the function, but not in the model.

# Derivate and Jacobian

Let  
\begin{align*}
\ff: \R^n \to \R^m
\end{align*}
be a differentiable function.
For $\zz \in \R^n$, the derivative $D\ff(\zz)$ is a linear map 
\begin{align*}
D\ff(\zz): \R^{n} \to \R^m,
\end{align*}
i.e., for all $\vv \in \R^n$, we have $D\ff(\zz) (\vv) \in \R^m$.
Linearity implies that the derivative can be identified with the **Jacobian matrix** 
\begin{align*}
J = D\ff(\zz) \in \R^{m \times n},
\end{align*}
so that  the evaluation $D\ff(\zz) (\vv)$ can be realized by the matrix-vector multiplication  $D\ff(\zz)_{m\times n} \vv_{n \times 1} \in \R^m$ with column vectors $\vv \in \R^n$.

The Jacobian $J$ is given by
\begin{align*}
    J =
\left( \left( \frac{\partial f_i(\zz)}{\partial z_j} \right)_{j=1}^m\right)_{i=1}^n 
    = \begin{bmatrix} \nabla f_1(\zz)^T \\ \vdots \\ \nabla f_m(\zz)^T \end{bmatrix}_{m \times n}
=
    \begin{bmatrix}
	\frac{\partial f_1}{\partial z_1} & \dots & \frac{\partial f_1}{\partial z_n}
	\\
	\frac{\partial f_2}{\partial z_1} & \dots & \frac{\partial f_2}{\partial z_n}
	\\
	\vdots
	\\
	\frac{\partial f_m}{\partial z_1} & \dots & \frac{\partial f_m}{\partial z_n}
    \end{bmatrix}_{m \times n}
    (\zz)
	= D\ff(\zz).
\end{align*}	

More generally, for a differentiable function $\ff: A \to B$, the derivative $D\ff(\aa)$ is a linear map between $A$ and $B$ for all $\aa \in A$.

For example, if $\ff: \R^n \to \R^m$ we have 
\begin{align*}
D\ff: \R^n \to \R^{m \times n}
\end{align*}
and therefore for any $\zz \in \R^n$, the second derivative $D (D\ff)(\zz) =: D^2 \ff(\zz)$ is a linear map
\begin{align*}
    D^2 \ff(\zz): \R^{m \times n} \to \R^n
\end{align*}	
which can be represented by a bilinear Hessian tensor, the elements of which can be computed by evaluating it on the tensor basis $\ee_i \otimes \ee_j \in \R^{m \times n}$

A special case when $\ff: \R^n \to \R^m$ is linear, there exists $A \in \R^{m\times n}$ such that $\ff(\zz) = A\zz$. In this case, $D\ff(\zz) = A$ for all $\zz \in \R^m$ and $D\ff(\zz) \vv = A\vv$ for all $\vv \in \R^n$.

## Chain rule

Let $\ff: \R^n \to \R^p$ and $\hh: \R^p \to \R^m$. Define $\FF : \R^n \to \R^m$ as $\FF(\xx) = \hh(\ff(\xx))$.
Then, chain rule implies
\begin{align*}
    [D\FF(\xx)]_{m \times n} = 
    [D\hh(\zz)|_{\zz = \ff(\xx)}  ]_{m \times p} [D\ff(\xx)]_{p \times n}
\end{align*}	
Defining $\vv$ as the **grad_output** (a common name in the pytorch literature)
\begin{align*}
    \vv = D\ff(\xx).
\end{align*}	
we can write
\begin{align*}
    D\FF(\xx) = D\hh(\ff(\xx)) (\vv).
\end{align*}	

In other words,
\begin{align*}
    J_{\hh \circ \ff} (\xx) = J_{\hh} (\ff(\xx)) J_{\ff}(\xx).
\end{align*}	






# Writing Custom `autograd` functions


When the loss function (or the model) involves composition with functions that are not simple, we need to specify the derivative manually. In fact, we do not need to specify the entire derivative tensor, just the tensor-vector product rule.

[Tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) gives some good examples.

In the `.backward()` method of a custom autograd function, given a vector $\vv \in \R^m$, we need to define the formula of $J\vv$. 
This is known as the **Jacobian-vector product**.
The vector $\vv$ is usually referred to as **grad_output(s)**.

The $i$-th element of $J\vv$ (for $i=1, \dots, n$) is
\begin{align*}
    (J \vv)_i = \sum_{k=1}^{m} \frac{\partial f_i}{\partial z_k} v_k.
\end{align*}	


### Example: forward difference operator

The forward difference operator (with Dirichlet or periodic boundary)
$\Delta^h : \R^n \to \R^n$ is given by
\begin{align*}
    \Delta^h_i (\zz) = \frac{1}{h} \left(  z_{i+1} - z_{i}\right).
\end{align*}	
Then, the partial derivative is
\begin{align*}
    \frac{\partial \Delta^h_i}{\partial z_k} = \frac{1}{h} \left( \delta_{i+1,k} - \delta_{i,k} \right)
\end{align*}	
where $\delta_{kj}$ is Kronecker's delta.
Therefore, for any $\vv \in \R^n$ we have
\begin{align*}
    (D_\zz \Delta^h(\zz) \vv)_i = \frac{1}{h}\sum_{k=1}^{n} (\delta_{i+1, k} - \delta_{i,k})v_k = \frac{1}{h} (v_{i+1} - v_i).
\end{align*}	


### Example: Legedre polynomial

If we are using an exotic function $g$ for which `pytorch` does not have the formula for derivate (and we do), we can define it ourselves

```python
class LegendrePolynomial3(torch.autograd.Function):
    """
    We can implement our own custom autograd Functions by subclassing
    torch.autograd.Function and implementing the forward and backward passes
    which operate on Tensors.
    """

    @staticmethod
    def forward(ctx, input):
        """
        In the forward pass we receive a Tensor containing the input and return
        a Tensor containing the output. ctx is a context object that can be used
        to stash information for backward computation. You can cache arbitrary
        objects for use in the backward pass using the ctx.save_for_backward method.
        """
        ctx.save_for_backward(input)
        return 0.5 * (5 * input ** 3 - 3 * input)

    @staticmethod
    def backward(ctx, grad_output):
        """
        In the backward pass we receive a Tensor containing the gradient of the loss
        with respect to the output, and we need to compute the gradient of the loss
        with respect to the input.
        """
        input, = ctx.saved_tensors
        return grad_output * 1.5 * (5 * input ** 2 - 1)
```

We can `apply` this function within the loop and define a loss function like this:

```python
# To apply our Function, we use Function.apply method. We alias this as 'P3'.
P3 = LegendrePolynomial3.apply
# Forward pass: compute predicted y using operations; we compute
# P3 using our custom autograd operation.
y_pred = a + b * P3(c + d * x)
# Compute and print loss
loss = (y_pred - y).pow(2).sum()
```




### Example: derivative operator


[This discussion](https://stackoverflow.com/questions/58839721/how-to-define-a-loss-function-in-pytorch-with-dependency-to-partial-derivatives) demonstrates a nice example of solving an ODE with initial data.

Computing numerical derivate of data with respect to another data can be done using `torch.autograd.grad()`.

Given $\ff: \R^n \to \R^m$, we have $D\ff(\zz): \R^n \to \R^m$ for all $\zz \in \R^n$. 

- If $m=1$, we can use `autograd.grad(output=f, input=z)` to compute $D\ff(\zz)$
- If $m \ge 1$, we can use `autograd.grad(output=f, input=z, grad_output=w)` to compute $D\ff(\zz)^T \ww$. For dimensional consistency, we need to have $\ww \in \R^m$ so that the output of `autograd.grad()` is in $\R^n$.

The [manual](https://pytorch.org/docs/stable/generated/torch.autograd.grad.html) says

```python
torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False)
```


Parameters

- `outputs`: The output tensors with respect to which gradients will be calculated. It has to be scalar valued, unless `grad_outputs` is specified, in which case, the returned value will be a vector-Jacobian product.
- `inputs`: The input tensors for which gradients are being computed. These must be part of the computational graph.
- `grad_outputs`: This parameter is an optional external gradient to be applied on `outputs`.
This is the vector $\vv$ in the vector-Jacobian product $\vv^TJ$. `None` values can be specified for scalar Tensors or ones that don’t require grad. If a `None` value would be acceptable for all grad_tensors, then this argument is optional. Default: `None`.
- `retain_graph`: When set to `True`, the computation graph used to compute the gradients will be retained, allowing for further operations.

- `create_graph`: If `True`, a new computational graph is created, enabling higher-order derivatives to be computed.
- `allow_unused`: If `True`, it returns `None` for input tensors unused in the computation; otherwise, it throws an error.


### Example: element-wise square

Consider the *element-wise square* function $\ff: \R^n \to \R^n$ given by $f_i(\xx) = x_i^2$ for all $i=1, \dots, n$.  Then, 
\begin{align*}
\frac{\partial f_i}{\partial x_k} = 2x_k \delta_{ik}.
\end{align*} 
The Jacobian is $J = $ Diag$(2x_1, \dots, 2x_n)$, a symmetric matrix.

To obtain the *element-wise double* operator $\hh: \R^n \to \R^n$ given by $h_i(\xx) = 2 x_i$ as a derivative of $\ff$, we can either compute $J^T\vv$ where $\vv = \1 \in \R^n$, a vector of ones ($\1_i =1$ for all $i=1, \dots, n$).


```python
import torch
x = torch.linspace(-10, 10, 5, requires_grad=True)
ones = torch.ones_like(x)
z = x **2
# Compute the derivatives
grads = torch.autograd.grad(outputs=z, inputs=x, grad_outputs=ones)

print('x', x)
print('ones', ones)
print('grads', grads)
```

which returns

```
x tensor([-10.,  -5.,   0.,   5.,  10.], requires_grad=True)
ones tensor([1., 1., 1., 1., 1.])
grads (tensor([-20., -10.,   0.,  10.,  20.]),)
```

The input data be a higher-order tensor. In that case, the `grad_output` has to be the same shape as `output` to return $D\ff(\zz)^T \vv$ where $\ff$=`output`, $\zz$=`input`, and $\vv$=`grad_output`.

```python
import torch
x = torch.linspace(-10, 10, 5, requires_grad=True)
x = x.reshape(-1,1).pow(torch.tensor([1, 2, 3]))
A = torch.tensor([[1, 1, 1, 1, 1], [1, 0, 1, 1, 1.]])
z = torch.matmul(A, x)
# Compute the derivatives
ones = torch.ones_like(z)
grads = torch.autograd.grad(outputs=z, inputs=x, grad_outputs=ones)

print('x', x)
print('ones', ones)
print('grads', grads)
```

returns

```
x tensor([[  -10.,   100., -1000.],
        [   -5.,    25.,  -125.],
        [    0.,     0.,     0.],
        [    5.,    25.,   125.],
        [   10.,   100.,  1000.]], grad_fn=<PowBackward1>)
z tensor([[  0., 250.,   0.],
        [  5., 225., 125.]], grad_fn=<MmBackward0>)
ones tensor([[1., 1., 1.],
        [1., 1., 1.]])
grads (tensor([[2., 2., 2.],
        [1., 1., 1.],
        [2., 2., 2.],
        [2., 2., 2.],
        [2., 2., 2.]]),)
```

By enabling `create_graph=True`, one can compute the higher-order derivates like this:

```python
a = torch.tensor(1.0, requires_grad=True)
t = torch.tensor(2.0, requires_grad=True)
b = a + 2 * t 
c = a * t + t ** 2
d = b**2 + 3 * c
grad_a, grad_t = torch.autograd.grad(outputs=d, inputs=(a, t), create_graph=True)
# Computing higher-order derivatives
second_order_grad_a = torch.autograd.grad(grad_a, a)[0]
second_order_grad_t = torch.autograd.grad(grad_t, t)[0]
```

This way, we can compute the (partial) derivatives of a data (`outputs`) with respect to grid points (`inputs`). This is applicable to solving ODEs with prescribed boundary and/or initial value.


---

# Tex and markdown conversion to html with pandoc

Look at these [many examples](https://github.com/mathjax/MathJax-demos-web?tab=readme-ov-file) for inspiration.

- To use latex goodies such as snippet completion etc in vim, set 

```vim
:setfiletype pandoc.tex
```


- Put all latex preamble in the header part of the `.md` file [source](https://pandoc.org/MANUAL.html#extension-yaml_metadata_block) like this

```yaml
---
title: Readme
author: Author
header-includes: |
    \usepackage{amsmath, amssymb, amsthm, amsfonts, color, bm}

    \newcommand{\R}{\mathcal{R}}
    \newcommand{\ww}{\boldsymbol{\omega}}
    \newcommand{\xx}{\mathbf{x}}
---
```

Converting a tex file into html `pandoc file.tex -o file.html` uses unicode by default to render math symbols. We need to use `mathjax` for a nicer rendering. Other options are

```
--mathml, --webtex, --mathjax, --katex
```

and demos can be found in pandoc [demos](https://pandoc.org/demos.html).

According to [pandoc documentation](https://pandoc.org/chunkedhtml-demo/3.6-math-rendering-in-html.html) One need to specify the url of the `.js` file that would be used to convert math into mathjax. By default pandoc uses some link form some content delivery network (CDN), which does not work on firefox at the first attempt. So we can specify the url like this:

```bash
pandoc math.text -s --mathjax=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js -o   mathMathJax.html
```

There are other CDN locations on [mathjax documentation](https://docs.mathjax.org/en/latest/web/start.html#cdn-list) from sites like

- jsdelivr.com [latest or specific version] (recommended)
- unpkg.com [latest or specific version]
- cdnjs.com
- raw.githack.com
- gitcdn.xyz
- cdn.statically.io

```bash
pandoc README.md -s --mathjax=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js -o   README.html
```

## LaTeX writing guide for MathJax

We want our `.md` file to convert to pdf or tex without much hassle. While using mathjax, a few things to remember for a quick conversion.

- Typically, in the header of any website using latex code, we need to add this script to load MathJax
 
```xml
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"> </script>
```

Here `tex-chtml-full` is a collection of components (combination of various packages and output font formats).

- You can load any out of the following components [link](https://docs.mathjax.org/en/latest/web/components/index.html)
    - tex-chtml
    - tex-chtml-full
    - tex-svg
    - tex-svg-full
    - tex-mml-chtml
    - tex-mml-svg
    - mml-chtml
    - mml-svg

- Various properties of the parser can be changed by adding a config script **before** the `.js` file is called within the `<script>` tag of the header of the `.md` file 

```yaml
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    tags: 'ams',              // equation numbering 'ams' or 'all'
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
```


- Equation numbering is turned off by default, it can be enabled using the config

```yaml
tex: {
    tags: 'ams',              // equation numbering 'ams' or 'all'
}
```

- Tex environments **not** enclosed by double `$$`s will be parsed by default as latex code. The  default option 

```
    processEnvironments: true, // process \begin{xxx}...\end{xxx} outside math mode
```


is responsible for this.

For example, 

    ```
    \begin{align*}
    x^2
    \end{align*}
    ```
is fine.

This is actually desired when converting `.md` to `.tex`. Otherwise, math environments enclosed by double `$$`s will throw error in the tex file. It needs to be remove manually before or after conversion before compiling latex. But developing the habit of writing math environments without the `$$`s  causes some pain since `vim-pandoc` does not render the math symbols within the environments in vim. 

- If you are using `\usepackage{bm}` for bold fonts and using commands like `\boldsymbol{\sigma}` etc (apparently better alternative to `\pmb{}`), make sure to put braces around it when using as subscript or superscript. For example, use `f_{\boldsymbol{\sigma}}(x)` instead of `f_\boldsymbol{\sigma}(x)`. The second usage will put `(x)` also within the subscript when you convert it into tex. This is a very weird behavior since you would think both are Tex commands.

- Bullet points: make sure to leave an empty line before the first top level bullet point. For sub-bullet points, no need to have empty line. Empty lines between bullet points makes the bullets more sparse.

## Diagrams

My observation is that using a freehand drawing tool like inkscape takes less time and provides more flexibility for creating diagrams. Markdown-friendly tools like `mermaid` required additional set up and does not seem to support latex within diagram. Maybe once can use latex `tikz` diagrams within markdown, but the whole point was to move away from programmable diagrams. Still:

Install pandoc filter for mermaid from [git](https://github.com/raghur/mermaid-filter?tab=readme-ov-file) using

```bash
npm install --global mermaid-filter
```

Use it with `-F` in pandoc

```
pandoc  -F mermaid-filter something.md -o something.html 
```

A sample block of code 

```pandoc
```{.mermaid format=png scale=5 caption='Computational graph'}
%%{init: {'theme':'neutral'}}%%
  graph TD
    a --> b
```
```

produces a flowchart-like diagram.

## Marp and usual markdown

Converting marp-focused `.md` file into html using `pandoc` is very glitchy. Few things that do not work so far

- Putting latex preambles to the yaml header
- Custom css for creating columns: math within the custom css does not render. Pandoc is not able to accept the marp theme css
- Bullet points render in a single line
- 

## Github readme compatibility

The markdown files with header does not render in github webpages, even though both are mathjax.

## Table tests

- So far, there seems to be 4 types of tables on markdown [see](https://pandoc.org/chunkedhtml-demo/8.9-tables.html). Here are some examples. Using a custom css, I can enhance the table displays


----------- ------- --------------- -------------------------
   First    row                12.0 Example of a row that
                                    spans multiple lines.

  Second    row                 5.0 Here's another one. Note
                                    the blank line between
                                    rows.

  Third     row                 5.0 Here's another one. Note
                                    the blank line between
                                    rows.

  Fourth    row                 5.0 Here's another one. Note
                                    the blank line between
                                    rows.
----------- ------- --------------- -------------------------

: Here's a multiline table without a header.



----------- ------- --------------- -------------------------
H1            H2          H3                  H4
----------- ------- --------------- -------------------------
   First    row                12.0 Example of a row that
                                    spans multiple lines.

  Second    row                 5.0 Here's another one. Note
                                    the blank line between
                                    rows.

  Third     row                 5.0 Here's another one. Note
                                    the blank line between
                                    rows.

  Fourth    row                 5.0 Here's another one. Note
                                    the blank line between
                                    rows.
----------- ------- --------------- -------------------------

: More table with header

+---------------------+-----------------------+
| Location            | Temperature 1961-1990 |
|                     | in degree Celsius     |
|                     +-------+-------+-------+
|                     | min   | mean  | max   |
+=====================+=======+=======+=======+
| Antarctica          | -89.2 | N/A   | 19.8  |
+---------------------+-------+-------+-------+
| Earth               | -89.2 | 14    | 56.7  |
+---------------------+-------+-------+-------+

: Here's a grid table


## Referencing
