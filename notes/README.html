<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Debdeep Bhattacharya" />
  <title>Notes on minimization problems using pytorch</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="/home/debdeep/Templates/pandoc-default-mod.css" />
  <!-- mathjax config goes before loading the js -->
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams',              // equation numbering 'ams' or 'all'
    },
    svg: {
      fontCache: 'global'
    }
  };
  </script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Notes on minimization problems using pytorch</h1>
<p class="author">Debdeep Bhattacharya</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#problem-setup" id="toc-problem-setup">Problem
setup</a></li>
<li><a href="#automatic-differentiation"
id="toc-automatic-differentiation">Automatic differentiation</a>
<ul>
<li><a href="#auto-differentiation-in-pytorch"
id="toc-auto-differentiation-in-pytorch">Auto-differentiation in
pytorch</a>
<ul>
<li><a href="#updating-the-parameters-variables-with-requires_gradtrue"
id="toc-updating-the-parameters-variables-with-requires_gradtrue">Updating
the parameters (variables with <code>requires_grad=True</code>)</a></li>
</ul></li>
<li><a href="#neural-network-approximation"
id="toc-neural-network-approximation">Neural network approximation</a>
<ul>
<li><a href="#higher-dimensional-formulation"
id="toc-higher-dimensional-formulation">Higher-dimensional
formulation</a></li>
<li><a href="#gradient-of-the-loss-function"
id="toc-gradient-of-the-loss-function">Gradient of the loss
function</a></li>
<li><a href="#example-of-a-neural-network-in-pytorch"
id="toc-example-of-a-neural-network-in-pytorch">Example of a neural
network in pytorch</a></li>
<li><a href="#custom-model-involving-neural-networks"
id="toc-custom-model-involving-neural-networks">Custom model involving
neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="#derivate-and-jacobian"
id="toc-derivate-and-jacobian">Derivate and Jacobian</a>
<ul>
<li><a href="#chain-rule" id="toc-chain-rule">Chain rule</a></li>
</ul></li>
<li><a href="#writing-custom-autograd-functions"
id="toc-writing-custom-autograd-functions">Writing Custom
<code>autograd</code> functions</a>
<ul>
<li><a href="#example-forward-difference-operator"
id="toc-example-forward-difference-operator">Example: forward difference
operator</a></li>
<li><a href="#example-legedre-polynomial"
id="toc-example-legedre-polynomial">Example: Legedre polynomial</a></li>
<li><a href="#example-derivative-operator"
id="toc-example-derivative-operator">Example: derivative
operator</a></li>
<li><a href="#example-element-wise-square"
id="toc-example-element-wise-square">Example: element-wise
square</a></li>
</ul></li>
<li><a href="#tex-and-markdown-conversion-to-html-with-pandoc"
id="toc-tex-and-markdown-conversion-to-html-with-pandoc">Tex and
markdown conversion to html with pandoc</a>
<ul>
<li><a href="#latex-writing-guide-for-mathjax"
id="toc-latex-writing-guide-for-mathjax">LaTeX writing guide for
MathJax</a></li>
<li><a href="#diagrams" id="toc-diagrams">Diagrams</a></li>
<li><a href="#marp-and-usual-markdown"
id="toc-marp-and-usual-markdown">Marp and usual markdown</a></li>
<li><a href="#github-readme-compatibility"
id="toc-github-readme-compatibility">Github readme
compatibility</a></li>
<li><a href="#table-tests" id="toc-table-tests">Table tests</a></li>
<li><a href="#referencing" id="toc-referencing">Referencing</a></li>
</ul></li>
</ul>
</nav>
<p>We will enhance the nice <a
href="https://pytorch.org/tutorials/beginner/pytorch_with_examples.html">pytorch
example</a> with extra explanations.</p>
<h1 id="problem-setup">Problem setup</h1>
<p>The goal is the fit <span class="math inline">\(\sin x\)</span> using
a cubic polynomial.</p>
<p>The data will be generated by sampling the curve <span
class="math inline">\(y = \sin x\)</span>. But this relationship between
<span class="math inline">\(x\)</span> and <span
class="math inline">\(y\)</span> will not be known to the modeler.</p>
<p>Let the data <span class="math inline">\(\{(x_i,
y_i)\}_{i=1}^n\)</span> be given.</p>
<p>Our model is <span class="math inline">\(y = f(x)\)</span> where
<span class="math display">\[\begin{align}
    \label{eq:model-cubic}
f_{\boldsymbol{\omega}}(x) = a + bx + cx^2 + d x^3,
\end{align}\]</span> where <span
class="math inline">\(\boldsymbol{\omega}= (a, b, c, d)\)</span> are the
free parameters to minimizer over.</p>
<p>The minimization problem is therefore</p>
<p><span class="math display">\[\begin{align}
    \label{eq:problem-minimization}
\min \left\{\sum_{i=1}^{n} \left\lvert y_i -
f_{\boldsymbol{\omega}}(x_i)\right\rvert^2: \boldsymbol{\omega}\in
\mathbb{R}^4 \right\}
\end{align}\]</span></p>
<p>To minimize, we use gradient descent method on <span
class="math inline">\(\eqref{eq:problem-minimization}\)</span>. Defining
the <em>loss function</em> (objective function) <span
class="math display">\[
L(\boldsymbol{\omega}) = \sum_{i=1}^{n} \left\lvert
f_{\boldsymbol{\omega}}(x_i) - y_i\right\rvert^2
\]</span></p>
<p>and then use the scheme <span class="math display">\[
\boldsymbol{\omega}_{n+1} = \boldsymbol{\omega}_n - \eta
\nabla_{\boldsymbol{\omega}} L(\boldsymbol{\omega}_n)
\]</span> with initial guess <span
class="math inline">\(\boldsymbol{\omega}_0\)</span> and <em>learning
rate</em> (numerical step size) <span
class="math inline">\(\eta\)</span>.</p>
<p>We would need to compute <span class="math display">\[
\nabla_{\boldsymbol{\omega}} L(\boldsymbol{\omega}) = 2 \sum_{i=1}^{n}
(f_{\boldsymbol{\omega}}(x_i) - y_i) \nabla_{\boldsymbol{\omega}}
f_{\boldsymbol{\omega}}(x_i)
\]</span> explicitly. For our model <span
class="math inline">\(f_{\boldsymbol{\omega}}\)</span>, noting that
<span class="math display">\[\begin{align*}
    \nabla_{\boldsymbol{\omega}} f_{\boldsymbol{\omega}}(x_i) =
\begin{bmatrix} 1 \\ x_i \\ x_i^2 \\ x_i^3 \end{bmatrix}
\end{align*}\]</span> we have <span
class="math display">\[\begin{align*}
    \frac{\partial L}{\partial a}(\boldsymbol{\omega}) &amp; = 2
\sum_{i=1}^{n} (f_{\boldsymbol{\omega}}(x_i) - y_i)
    \\
    \frac{\partial L}{\partial b}(\boldsymbol{\omega}) &amp;= 2
\sum_{i=1}^{n} (f_{\boldsymbol{\omega}}(x_i) - y_i) x_i
    \\
    \frac{\partial L}{\partial c}(\boldsymbol{\omega}) &amp;= 2
\sum_{i=1}^{n} (f_{\boldsymbol{\omega}}(x_i) - y_i) x_i^2
    \\
    \frac{\partial L}{\partial d}(\boldsymbol{\omega}) &amp;= 2
\sum_{i=1}^{n} ( f_{\boldsymbol{\omega}}(x_i) - y_i) x_i^3
\end{align*}\]</span></p>
<p>In summary, <span class="math display">\[\begin{align*}
    \nabla_{\boldsymbol{\omega}} L(\boldsymbol{\omega}_n) =
\begin{bmatrix} \mathbf{u}\cdot \mathcal{1} \\ \mathbf{u}\cdot
\mathbf{x}\\ \mathbf{u}\cdot \mathbf{x}^2 \\ \mathbf{u}\cdot
\mathbf{x}^3 \end{bmatrix}
\end{align*}\]</span> where <span class="math inline">\(\mathcal{1} \in
\mathbb{R}^n\)</span> is a vector of ones, <span
class="math inline">\(\mathbf{x}^n \in \mathbb{R}^n\)</span> is
elementwise <span class="math inline">\(n\)</span>-th power of <span
class="math inline">\(\mathbf{x}= (x_i)_{i=1}^n\)</span>, and <span
class="math inline">\(\mathbf{u}= 2 (f_{\boldsymbol{\omega}_n}(x_i) -
y_i)_{i=1}^n\)</span>.</p>
<p>This is done in the following python code:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create random input and output data</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span>math.pi, math.pi, <span class="dv">2000</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.sin(x)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly initialize weights</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> np.random.randn()</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.random.randn()</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> np.random.randn()</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> np.random.randn()</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass: compute predicted y</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># y = a + b x + c x^2 + d x^3</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> a <span class="op">+</span> b <span class="op">*</span> x <span class="op">+</span> c <span class="op">*</span> x <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> d <span class="op">*</span> x <span class="op">**</span> <span class="dv">3</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute and print loss</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> np.square(y_pred <span class="op">-</span> y).<span class="bu">sum</span>()</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> t <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">99</span>:</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(t, loss)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backprop to compute gradients of a, b, c, d with respect to loss</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    grad_y_pred <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> (y_pred <span class="op">-</span> y)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    grad_a <span class="op">=</span> grad_y_pred.<span class="bu">sum</span>()</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    grad_b <span class="op">=</span> (grad_y_pred <span class="op">*</span> x).<span class="bu">sum</span>()</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    grad_c <span class="op">=</span> (grad_y_pred <span class="op">*</span> x <span class="op">**</span> <span class="dv">2</span>).<span class="bu">sum</span>()</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    grad_d <span class="op">=</span> (grad_y_pred <span class="op">*</span> x <span class="op">**</span> <span class="dv">3</span>).<span class="bu">sum</span>()</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update weights</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    a <span class="op">-=</span> learning_rate <span class="op">*</span> grad_a</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    b <span class="op">-=</span> learning_rate <span class="op">*</span> grad_b</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    c <span class="op">-=</span> learning_rate <span class="op">*</span> grad_c</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    d <span class="op">-=</span> learning_rate <span class="op">*</span> grad_d</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Result: y = </span><span class="sc">{</span>a<span class="sc">}</span><span class="ss"> + </span><span class="sc">{</span>b<span class="sc">}</span><span class="ss"> x + </span><span class="sc">{</span>c<span class="sc">}</span><span class="ss"> x^2 + </span><span class="sc">{</span>d<span class="sc">}</span><span class="ss"> x^3&#39;</span>)</span></code></pre></div>
<h1 id="automatic-differentiation">Automatic differentiation</h1>
<p>The minimization problems are set up using the loss function, which
is a <strong>composition</strong> of several functions <span
class="math inline">\(g^1: \mathbb{R}^4 \to \mathbb{R}^n\)</span>, <span
class="math inline">\(g^2: \mathbb{R}^n \to \mathbb{R}^n\)</span>, and
<span class="math inline">\(g^3: \mathbb{R}^n \to
\mathbb{R}\)</span></p>
<p><span class="math display">\[\begin{align*}
    g^1(\boldsymbol{\omega}) &amp;= (y_i -
f_{\boldsymbol{\omega}}(x_i))_{i=1}^n
\\
    g^2(\mathbf{x}) &amp;= (x_i^2)_{i=1}^n
\\
    g^3(\mathbf{x}) &amp;= \sum_{i=1}^{n} x_i
\end{align*}\]</span> Then, <span class="math display">\[
L(\boldsymbol{\omega}) = g^3(g^2(g^1(\boldsymbol{\omega})))
\]</span> Therefore, the <span class="math inline">\(k\)</span>-th
partial derivative of the loss function is</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial}{\partial \omega_k} L(\boldsymbol{\omega})
    &amp; = \sum_{i=1}^{n} \frac{\partial }{\partial x_i}
g^3(\mathbf{x})|_{\mathbf{x}= g^2(g^1(\boldsymbol{\omega}))}
\frac{\partial}{\partial \omega_k}  g^2_i(g^1(\boldsymbol{\omega}))
\\
    &amp; = \sum_{i=1}^{n} \frac{\partial }{\partial x_i}
g^3(\mathbf{x})|_{\mathbf{x}= g^2(g^1(\boldsymbol{\omega}))}
\sum_{j=1}^{n} \frac{\partial}{\partial
x_j}  g^2_i(\mathbf{x})|_{\mathbf{x}= g^1(\boldsymbol{\omega})}
\frac{\partial}{\partial \omega_k}  g^1_j(\boldsymbol{\omega})
\end{align*}\]</span></p>
<p>More generally, if <span class="math inline">\(\boldsymbol{\omega}\in
\mathbb{R}^p\)</span> (i.e., <span class="math inline">\(p\)</span>
parameters to minimize), and if <span class="math inline">\(L\)</span>
is a composition of <span class="math inline">\(M\)</span> functions
<span class="math display">\[
L(\boldsymbol{\omega}) = (g^{M} \circ g^{M-1} \circ \dots \circ
g^1)(\boldsymbol{\omega}),
\]</span> where <span class="math inline">\(g^i : \mathbb{R}^{p_{i-1}}
\to \mathbb{R}^{p_i}\)</span> for all <span class="math inline">\(i=1,
\dots, M\)</span>.</p>
<p>For dimensional consistency, we must have <span
class="math inline">\(p_0 =0\)</span> and <span
class="math inline">\(p_M = 1\)</span>. Therefore, we have <span
class="math inline">\(g^1: \mathbb{R}^p \to \mathbb{R}^{p_1}\)</span>,
<span class="math inline">\(g^2: \mathbb{R}^{p_1} \to \mathbb{R}^{p_2},
\dots, g^{M-1}: \mathbb{R}^{p_{M-2}} \to \mathbb{R}^{p_{M-1}}\)</span>,
<span class="math inline">\(g^M: \mathbb{R}^{p_{M-1}} \to
\mathbb{R}\)</span>.</p>
<p>Differentiating with respect to the parameter <span
class="math inline">\(w_k\)</span>, we can write <span
class="math display">\[\begin{align*}
    \frac{\partial}{\partial \omega_k} L(\boldsymbol{\omega})  
    = &amp;  
    \sum_{i_{M-1}=1}^{p_{M-1}}
    \frac{\partial }{\partial x_{i_{M-1}}} g^M(\mathbf{x}^{M-1})
    \sum_{i_{M-2}=1}^{p_{M-2}}
    \frac{\partial }{\partial x_{i_{M-2}}}
g_{i_{M-1}}^{M-1}(\mathbf{x}^{M-2})
    \dots
    \\
    &amp;
    \sum_{i_{M_1}=1}^{p_1}
    \frac{\partial }{\partial x_{i_{M_1}}} g_{i_{M_2}}^{2}(\mathbf{x}^1)
    % \sum_{i_{M_1}=1}^{p_1}
    \frac{\partial }{\partial \omega_{k}}
g_{i_{M_1}}^{1}(\boldsymbol{\omega})
    \\
    = &amp;  
    \sum_{i_{M-1}=1}^{p_{M-1}}
    \sum_{i_{M-2}=1}^{p_{M-2}}
    \dots
    \sum_{i_{M_1}=1}^{p_1}
    \frac{\partial }{\partial x_{i_{M-1}}} g^M(\mathbf{x}^{M-1})
    \frac{\partial }{\partial x_{i_{M-2}}}
g_{i_{M-1}}^{M-1}(\mathbf{x}^{M-2})
    \dots
    \\
    &amp;
    \frac{\partial }{\partial x_{i_{M_1}}} g_{i_{M_2}}^{2}(\mathbf{x}^1)
    % \sum_{i_{M_1}=1}^{p_1}
    \frac{\partial }{\partial \omega_{k}}
g_{i_{M_1}}^{1}(\boldsymbol{\omega})
    \\
\end{align*}\]</span> where <span
class="math inline">\(\mathbf{x}^i\)</span> is defined as <span
class="math inline">\((g^i \circ g^{i-1} \circ \dots \circ g^1)
(\boldsymbol{\omega})\)</span> for each <span class="math inline">\(i=1,
2, \dots, M-1\)</span>.</p>
<p>Therefore, to compute <span
class="math inline">\(\nabla_{\boldsymbol{\omega}}
L(\boldsymbol{\omega})\)</span>, one needs to know the tensor <span
class="math display">\[
d_{rst} = \frac{\partial}{\partial x_r} g^s_t(\mathbf{x}^{s-1})
\]</span> for all <span class="math inline">\(s=1, \dots, M\)</span>,
<span class="math inline">\(t=1, \dots, p_s\)</span>, and <span
class="math inline">\(r=1, \dots, p_{s-1}\)</span>. Combining all <span
class="math inline">\(d_{rst}\)</span> to compute <span
class="math inline">\(\nabla_{\boldsymbol{\omega}}
L(\boldsymbol{\omega})\)</span> is know as <em>backward
propagation</em>.</p>
<p>Note that the tensor <span class="math inline">\(d_{rst}\)</span>
describes the <span class="math inline">\(r\)</span>-th partial
derivative of <span class="math inline">\(t\)</span>-th component of the
function <span class="math inline">\(g^s\)</span>, evaluated at the
immediate value <span
class="math inline">\(\mathbf{x}^{s-1}\)</span>.</p>
<p>For notational convenience, one could use Einstein’s convention for
summation (repeated entries are summed) and simply write <span
class="math display">\[\begin{align*}
\frac{\partial}{\partial w_k}L(\boldsymbol{\omega})
    =
    \prod_{s=2}^{M} \prod_{t=1}^{p_s} \prod_{r=1}^{p_{s-1}}
    \frac{\partial}{\partial x_r} g^s_t(\mathbf{x}^{s-1})
    \frac{\partial}{\partial w_k} g^1_{i_{M_1}}(\boldsymbol{\omega})
\end{align*}\]</span> but we do not adapt this notation here.</p>
<p>The graph-theoretic realization of this procedure is a directed graph
from left to right with leaves on the left as <span
class="math inline">\(\omega_k\)</span>. Edges of the graph represent
the partial derivates of the target nodes with respect to the source
nodes evaluated at the value of the source node. The last node on the
right is the loss function <span class="math inline">\(L\)</span> In
this setup, back-propagation procedure populates the leaf nodes <span
class="math inline">\(\omega_k\)</span> with <span
class="math inline">\(\frac{\partial}{\partial \omega_k}
L(\boldsymbol{\omega})\)</span>.</p>
<h2 id="auto-differentiation-in-pytorch">Auto-differentiation in
pytorch</h2>
<p>Let <span class="math inline">\(\boldsymbol{\omega}= (a, b, c, d) \in
\mathbb{R}^4\)</span>.</p>
<p>In <code>pytorch</code>, setting <code>requires_grad=True</code>
while defining a variable <span class="math inline">\(a\)</span> implies
we would want to compute <span
class="math inline">\(\frac{\partial}{\partial a}\)</span> of a function
of <span class="math inline">\(a\)</span> at some point.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Setting requires_grad=True indicates that we want to compute gradients with</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># respect to these Tensors during the backward pass.</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.randn((), dtype<span class="op">=</span>dtype, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn((), dtype<span class="op">=</span>dtype, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> torch.randn((), dtype<span class="op">=</span>dtype, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> torch.randn((), dtype<span class="op">=</span>dtype, requires_grad<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>Given (fixed) data <code>x</code> and <code>y</code>, we define a
loss function <span
class="math inline">\(L(\boldsymbol{\omega})\)</span> called
<code>loss</code></p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> a <span class="op">+</span> b <span class="op">*</span> x <span class="op">+</span> c <span class="op">*</span> x <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> d <span class="op">*</span> x <span class="op">**</span> <span class="dv">3</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> (y_pred <span class="op">-</span> y).<span class="bu">pow</span>(<span class="dv">2</span>).<span class="bu">sum</span>()</span></code></pre></div>
<p><strong>Remark:</strong> Any variable such as <code>y_pred</code> and
<code>loss</code> defined as a function of variables with
<code>requires_grad=True</code> (such as <code>a, b, c, d</code>), will
automatically have <code>requires_grad=True</code>. This feature makes
sure that a computational graph gets created to store the subsequent
partial derivatives <span class="math inline">\(d_{rst}\)</span>. To
define <em>inferential</em> variables (variables you do not plan to
compute partial derivative of) you need to turn this off manually using
<code>torch.no_grad()</code> like this</p>
<pre><code>&gt;&gt;&gt; with torch.no_grad():
...     y = x * 2
&gt;&gt;&gt; y.requires_grad
False

&gt;&gt;&gt; @torch.no_grad()
... def tripler(x):
...     return x * 3
&gt;&gt;&gt; z = tripler(x)
&gt;&gt;&gt; z.requires_grad</code></pre>
<p>At some point during our computation, we will need to compute the
partial derivative <span class="math inline">\(\frac{\partial}{\partial
a}L(\boldsymbol{\omega}^n)\)</span> using the current value of <span
class="math inline">\(\boldsymbol{\omega}=
\boldsymbol{\omega}^n\)</span>. In fact, we can compute all the partial
derivatives <span class="math inline">\(\nabla_{\boldsymbol{\omega}}
L(\boldsymbol{\omega}^n)\)</span> at once by calling the
<code>backward()</code> function on the objective function <span
class="math inline">\(L\)</span> (<code>loss</code>) like this:</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use autograd to compute the backward pass. This call will compute the</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># gradient of loss with respect to all Tensors with requires_grad=True.</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># the gradient of the loss with respect to a, b, c, d respectively.</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>loss.backward()</span></code></pre></div>
<p>At this point, all partial derivates of <code>loss</code> function
<span class="math inline">\(L\)</span> with respect to all variables
with a <code>requires_grad=True</code> is computed at the current value
of <span class="math inline">\(\boldsymbol{\omega}=
\boldsymbol{\omega}^n\)</span>. We can get the value of <span
class="math inline">\(\frac{\partial}{\partial
a}L(\boldsymbol{\omega}^n)\)</span> at the current value <span
class="math inline">\(\boldsymbol{\omega}=
\boldsymbol{\omega}^n\)</span> using <code>a.grad</code>.</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>a.grad</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>b.grad</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>c.grad</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>d.grad</span></code></pre></div>
<h3
id="updating-the-parameters-variables-with-requires_gradtrue">Updating
the parameters (variables with <code>requires_grad=True</code>)</h3>
<p><span class="math display">\[\frac{\partial}{\partial
a}L(\boldsymbol{\omega}^n)
\to \frac{\partial}{\partial a}L(\boldsymbol{\omega}^{n+1})\]</span></p>
<p>Note that we would want to update the value of <span
class="math inline">\(\boldsymbol{\omega}\)</span> (in particular, the
value of <span class="math inline">\(a\)</span>) from <span
class="math inline">\(\boldsymbol{\omega}^n\)</span> to <span
class="math inline">\(\boldsymbol{\omega}^{n+1}\)</span>, for example,
during a gradient descent method. This should change <span
class="math inline">\(\frac{\partial}{\partial
a}L(\boldsymbol{\omega}^n)\)</span>, but the update does not happen
unless you run <code>loss.backward()</code> again.</p>
<p><strong>Remark:</strong> While manually updating variables with
<code>requires_grad=True</code>, we turn off gradient computation
(why?). We would recompute the gradient again anyway, and do not want to
spend computational power computing the gradients of update functions.
Therefore, do the following:</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Manually update weights using gradient descent. Wrap in torch.no_grad()</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># because weights have requires_grad=True, but we don&#39;t need to track this</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># in autograd.</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    a <span class="op">-=</span> learning_rate <span class="op">*</span> a.grad</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    b <span class="op">-=</span> learning_rate <span class="op">*</span> b.grad</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    c <span class="op">-=</span> learning_rate <span class="op">*</span> c.grad</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    d <span class="op">-=</span> learning_rate <span class="op">*</span> d.grad</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Manually zero the gradients after updating weights</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    a.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    b.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    c.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    d.grad <span class="op">=</span> <span class="va">None</span></span></code></pre></div>
<h2 id="neural-network-approximation">Neural network approximation</h2>
<p>Consider the function <span class="math inline">\(f_d\)</span>
representing the output of a convolutional neural network of depth <span
class="math inline">\(d \ge 1\)</span> with <em>activation function</em>
<span class="math inline">\(\sigma\)</span> defined recursively by <span
class="math display">\[\begin{align*}
    f_1(x) &amp;= \sigma(a_1 x + b_1)
    \\
    f_2(x) &amp;= \sigma(a_2 f_1(x) + b_2)
    \\
    \vdots
    \\
    f_d(x) &amp;= \sigma(a_d f_{d-1}(x) + b_d).
\end{align*}\]</span></p>
<p>Few common choices of <span class="math inline">\(\sigma(x)\)</span>
are <span class="math inline">\(x\)</span>, <span
class="math inline">\(\tanh(x)\)</span>, Heaviside, and logistic
function. The neural network model is therefore <span
class="math display">\[\begin{align*}
    y = f_d(x)
\end{align*}\]</span> where <span class="math inline">\(a_1, \dots,
a_d\)</span> and <span class="math inline">\(b_1, \dots, b_d\)</span>
are the parameters of the model. The corresponding <span
class="math inline">\(l^2\)</span> minimization problem is <span
class="math display">\[\begin{align*}
    \min_{a_1, \dots, a_d, b_1, \dots, b_d \in \mathbb{R}}
\sum_{i=1}^{n} \left\lvert y_i - f_d(x_i)\right\rvert^2.
\end{align*}\]</span></p>
<p>There is no general closed-form solution to the minimization problem.
Methods like gradient descent can be use to find a minimizer
numerically. Consider the function <span
class="math inline">\(f_d\)</span> representing the output of a
convolutional neural network of depth <span class="math inline">\(d \ge
1\)</span> with <em>activation function</em> <span
class="math inline">\(\sigma\)</span> defined recursively by <span
class="math display">\[\begin{align*}
    f_1(x) &amp;= \sigma(a_1 x + b_1)
    \\
    f_2(x) &amp;= \sigma(a_2 f_1(x) + b_2)
    \\
    \vdots
    \\
    f_d(x) &amp;= \sigma(a_d f_{d-1}(x) + b_d).
\end{align*}\]</span> Few common choices of <span
class="math inline">\(\sigma(x)\)</span> are <span
class="math inline">\(x\)</span>, <span
class="math inline">\(\tanh(x)\)</span>, Heaviside, and logistic
function. The neural network model is therefore <span
class="math display">\[\begin{align*}
    y = f_d(x)
\end{align*}\]</span><br />
where <span class="math inline">\(a_1, \dots, a_d\)</span> and <span
class="math inline">\(b_1, \dots, b_d\)</span> are the parameters of the
model. The corresponding <span class="math inline">\(l^2\)</span>
minimization problem is <span class="math display">\[\begin{align*}
    \min_{a_1, \dots, a_d, b_1, \dots, b_d \in \mathbb{R}}
\sum_{i=1}^{n} \left\lvert y_i - f_d(x_i)\right\rvert^2.
\end{align*}\]</span></p>
<p>There is no general closed-form solution to the minimization problem.
Methods like gradient descent can be use to find a minimizer
numerically.</p>
<h3 id="higher-dimensional-formulation">Higher-dimensional
formulation</h3>
<p>Let the <span class="math inline">\(i\)</span>-th data point be <span
class="math inline">\((\mathbf{x}_i, \mathbf{y}_i)\)</span>, where <span
class="math inline">\(\mathbf{x}_i \in \mathbb{R}^l\)</span> (i.e. data
has <span class="math inline">\(p\)</span> <em>features</em> and <span
class="math inline">\(q\)</span> labels).</p>
<p>An affine function <span class="math inline">\(g_{W, \mathbf{b}}:
\mathbb{R}^p \to \mathbb{R}^s\)</span> is defined by <span
class="math display">\[\begin{align*}
    g_{W, \mathbf{b}}(\mathbf{x}) = W\mathbf{x}+ \mathbf{b},
\end{align*}\]</span><br />
where <span class="math inline">\(W \in \mathbb{R}^{s\times p}\)</span>
is called the <em>weight</em> and <span
class="math inline">\(\mathbf{b}\in \mathbb{R}^s\)</span> is called the
<em>bias</em>.</p>
<p>An <em>activation function</em> <span class="math inline">\(\sigma:
\mathbb{R}\to \mathbb{R}\)</span> is a differentiable, nonlinear
function. Acting element-wise, with a slight abuse of notation, we
define <span class="math inline">\(\sigma: \mathbb{R}^n \to
\mathbb{R}^n\)</span> by <span class="math display">\[\begin{align*}
    \sigma(\mathbf{x}) = (\sigma(x_i))_{i=1}^n.
\end{align*}\]</span></p>
<p>A <em>single linear neural layer</em> <span
class="math inline">\(f:\mathbb{R}^p \to \mathbb{R}^s\)</span> with
weight <span class="math inline">\(W \in \mathbb{R}^{s \times
p}\)</span>, bias <span class="math inline">\(\mathbf{b}\in
\mathbb{R}^s\)</span>, and activation function <span
class="math inline">\(\sigma: \mathbb{R}^s \to \mathbb{R}^s\)</span> is
defined as <span class="math display">\[\begin{align*}
f = \sigma \circ g.
\end{align*}\]</span></p>
<p>A depth-<span class="math inline">\(d\)</span> neural network
<em>model</em> is therefore given by <span
class="math display">\[\begin{align*}
    N^d(\mathbf{x}) = (f^d \circ \dots \circ f^1) (\mathbf{x})
\end{align*}\]</span><br />
where for all <span class="math inline">\(i=1, \dots, d\)</span>, <span
class="math display">\[\begin{align*}
    f^i = \sigma^i \circ g^i,
\end{align*}\]</span><br />
where <span class="math inline">\(g^i\)</span> is a linear neural
network and <span class="math inline">\(\sigma^i\)</span> is an
activation function. Let for all <span class="math inline">\(i=1, \dots,
d\)</span>, we have <span class="math inline">\(g^i:
\mathbb{R}^{p_{i-1}} \to \mathbb{R}^{p_{i}}\)</span> with weight <span
class="math inline">\(W^i \in \mathbb{R}^{p_{i} \times p_{i-1}}\)</span>
and bias <span class="math inline">\(\mathbf{b}^i \in
\mathbb{R}^{p_i}\)</span>.</p>
<p>For dimensional consistency, we must have <span
class="math inline">\(p_0 = p\)</span> and <span
class="math inline">\(p_d=q\)</span>.</p>
<p>A loss function <span class="math inline">\(L\)</span> over <span
class="math inline">\(n\)</span> data points <span
class="math inline">\(\{ (\mathbf{x}_i, \mathbf{y}_i)
\}_{i=1}^n\)</span> is a function <span
class="math display">\[\begin{align*}
    L(W^d, \dots, W^1, \mathbf{b}^d, \dots, \mathbf{b}^1)
=  \sum_{i=1}^{n} \left\lVert N^d(\mathbf{x}_i) -
\mathbf{y}_i\right\rVert^2.
\end{align*}\]</span></p>
<h3 id="gradient-of-the-loss-function">Gradient of the loss
function</h3>
<p>To compute the gradient of the loss function with respect to each
bias <span class="math inline">\(b^i_j\)</span> and weight <span
class="math inline">\(w^i_{jk}\)</span>, recalling <span
class="math display">\[\begin{align*}
    g^i_l(\mathbf{z}) = \sum_{s=1}^{p_{m-1}} W^i_{ls} z_s + b^i_l
\end{align*}\]</span><br />
we need the derivatives <span class="math display">\[\begin{align*}
    \frac{d}{db^i_{j}}g^m_l(\mathbf{z}) =
    \frac{\partial}{\partial b^i_{j}}  b^m_l =
     \delta_{mi} \delta_{lj}
\end{align*}\]</span><br />
and <span class="math display">\[\begin{align*}
    \frac{d}{dw^i_{jk}}g^m_l(\mathbf{z})
    &amp;=
    \frac{\partial}{\partial w^i_{jk}} \left(\sum_{s=1}^{p_{m-1}}
W^m_{ls} z_s + b^m_l\right)
    \\
    &amp;= \sum_{s=1}^{p_{m-1}} \delta_{mi} \delta_{lj}\delta_{sk} z_s
    \\
    &amp;= \delta_{mi} \delta_{lj} z_k,
\end{align*}\]</span><br />
where <span class="math inline">\(\delta_{kj}\)</span> denotes the
Kronecker’s delta <span class="math display">\[\begin{align*}
    \delta_{kj} =
    \begin{cases}
       1, &amp;\text{ if } k = j
       \\
       0, &amp;\text{ otherwise }.
    \end{cases}
\end{align*}\]</span></p>
<p>The gradient of the element-wise activation function <span
class="math inline">\(\sigma^m\)</span> is <span
class="math display">\[\begin{align*}
    \frac{\partial}{\partial z_j} \sigma^m_k(\mathbf{z}) =
    \frac{\partial}{\partial z_j} \sigma^m(z_k) = (\sigma^m)&#39;(z_k)
\delta_{kj}.
\end{align*}\]</span></p>
<p>Therefore, the partial derivatives of each linear neural layer <span
class="math inline">\(f^m\)</span> are <span
class="math display">\[\begin{align*}
    \frac{d}{db^i_{j}}f^m_l(\mathbf{z})
    &amp;=
    \frac{d}{db^i_{j}} \sigma^m_l(g^m(\mathbf{z}))
    \\
    &amp;= \sum_{k=1}^{p_{m-1}} \frac{\partial}{\partial z_k}
\sigma^m_l(g^m(\mathbf{z})) \frac{\partial}{\partial b^i_j}
g^m_k(\mathbf{z})
    \\
    &amp; =
    \sum_{k=1}^{p_{m-1}}
    (\sigma^m)&#39;(g^m_l(\mathbf{z})) \delta_{lk}
    \delta_{mi} \delta_{kj}
    \\
    &amp; =
    (\sigma^m)&#39;(g^m_l(\mathbf{z}))
    \delta_{mi} \delta_{lj}
\end{align*}\]</span><br />
and <span class="math display">\[\begin{align*}
    \frac{d}{dw^i_{jk}}f^m_l(\mathbf{z})
    &amp;= \sum_{s=1}^{p_{m-1}} \frac{\partial}{\partial z_s}
\sigma^m_l(g^m(\mathbf{z})) \frac{\partial}{\partial w^i_{jk}}
g^m_s(\mathbf{z})
    \\
    &amp;=
    \sum_s
    (\sigma^m)&#39;(g^m_l(\mathbf{z})) \delta_{ls}
    \delta_{mi} \delta_{sj} z_k
    \\
    &amp;=
    (\sigma^m)&#39;(g^m_l(\mathbf{z})) z_k
    \delta_{mi} \delta_{lj} .
\end{align*}\]</span></p>
<h3 id="example-of-a-neural-network-in-pytorch">Example of a neural
network in pytorch</h3>
<p>We construct a neural network <span
class="math inline">\(N^d\)</span> of depth <span
class="math inline">\(d=2\)</span> for data with <span
class="math inline">\(p = 4\)</span> features and <span
class="math inline">\(q=1\)</span> label(s). We consider <span
class="math display">\[\begin{align*}
g^1: \mathbb{R}^4 \to \mathbb{R}^8
\\
g^2: \mathbb{R}^8 \to \mathbb{R}^1
\end{align*}\]</span><br />
with <span class="math inline">\(\sigma^1(r) = \max\{0, r\}\)</span>
(called ReLU(<span class="math inline">\(r\)</span>)) and <span
class="math inline">\(\sigma^2(r) = r\)</span>. The affine maps are
defined as <span class="math display">\[\begin{align*}
    g^i(\mathbf{z}) = W^i \mathbf{z}+ b^i
\end{align*}\]</span><br />
for each <span class="math inline">\(\mathbf{z}\in\)</span> Domain(<span
class="math inline">\(g^i\)</span>) for <span
class="math inline">\(i=1,2\)</span>. Then, the depth-2 neural network
<span class="math inline">\(N^d\)</span> is defined as <span
class="math display">\[\begin{align*}
    N^d(\mathbf{x}) = (g^2 \circ \sigma^1 \circ g^1) (\mathbf{x})
\end{align*}\]</span></p>
<p>The weights and biases of each affine map are <span
class="math inline">\(W^1 \in \mathbb{R}^{8 \times 4}, W^2 \in
\mathbb{R}^{1 \times 8}\)</span> and <span
class="math inline">\(\mathbf{b}^1 \in \mathbb{R}^8\)</span> and <span
class="math inline">\(\mathbf{b}^2 \in \mathbb{R}^1\)</span>.</p>
<p>For each of <span class="math inline">\(\mathbf{x}_i\)</span> of the
data set <span class="math inline">\(\{ (\mathbf{x}_i, y_i)
\}_{i=1}^3\)</span>, we compute <span
class="math inline">\(N^d(\mathbf{x}_i)\)</span>. In the following code,
<code>X</code> = <span
class="math inline">\((\mathbf{x}_i)_{i=1}^3\)</span> and
<code>net(X)</code> = <span
class="math inline">\((N^d(\mathbf{x}_i))_{i=1}^3\)</span>.</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> nn.Sequential( </span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>                    nn.Linear(<span class="dv">4</span>,<span class="dv">8</span>),</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>                    nn.ReLU(),</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>                    nn.Linear(<span class="dv">8</span>,<span class="dv">1</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.rand(size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">4</span>))</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;X&#39;</span>, X)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;net(X)&#39;</span>, net(X))</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co">## Use net.state_dict() to show the parameters</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Show parameters of the first neural network in the sequence:&#39;</span>, net[<span class="dv">0</span>].state_dict())</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Show parameters of the third neural network in the sequence:&#39;</span>, net[<span class="dv">2</span>].state_dict())</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;print the bias data of 3rd net:&#39;</span>, net[<span class="dv">2</span>].bias.data)</span></code></pre></div>
<h3 id="custom-model-involving-neural-networks">Custom model involving
neural networks</h3>
<p>To use optimized pytorch features such as
<code>torch.nn.MSELoss()</code>, <code>torch.optim.SGD()</code>,
<code>optimizer.step()</code> etc for your own model, you need to define
your own model as a module, which is a derived class of
<code>torch.nn.Module</code>.</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Polynomial3(torch.nn.Module):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co">        In the constructor we instantiate four parameters and assign them as</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">        member parameters.</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.a <span class="op">=</span> torch.nn.Parameter(torch.randn(()))</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b <span class="op">=</span> torch.nn.Parameter(torch.randn(()))</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c <span class="op">=</span> torch.nn.Parameter(torch.randn(()))</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d <span class="op">=</span> torch.nn.Parameter(torch.randn(()))</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co">        In the forward function we accept a Tensor of input data and we must return</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co">        a Tensor of output data. We can use Modules defined in the constructor as</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co">        well as arbitrary operators on Tensors.</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.a <span class="op">+</span> <span class="va">self</span>.b <span class="op">*</span> x <span class="op">+</span> <span class="va">self</span>.c <span class="op">*</span> x <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="va">self</span>.d <span class="op">*</span> x <span class="op">**</span> <span class="dv">3</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> string(<span class="va">self</span>):</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="co">        Just like any class in Python, you can also define custom method on PyTorch modules</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f&#39;y = </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>a<span class="sc">.</span>item()<span class="sc">}</span><span class="ss"> + </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>b<span class="sc">.</span>item()<span class="sc">}</span><span class="ss"> x + </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>c<span class="sc">.</span>item()<span class="sc">}</span><span class="ss"> x^2 + </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>d<span class="sc">.</span>item()<span class="sc">}</span><span class="ss"> x^3&#39;</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct our model by instantiating the class defined above</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Polynomial3()</span></code></pre></div>
<p>Note that we do not need to define a <code>backward()</code> method
for a <code>torch.nn.Module</code> as it is derived from the operations
specified within the <code>forward()</code> method. If your model
(<code>torch.nn.Module</code>) uses an exotic function which you define
via a <code>torch.autograd.Function</code>, then you define the derivate
of the function within the <code>backward()</code> method of the
function, but not in the model.</p>
<h1 id="derivate-and-jacobian">Derivate and Jacobian</h1>
<p>Let<br />
<span class="math display">\[\begin{align*}
\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m
\end{align*}\]</span> be a differentiable function. For <span
class="math inline">\(\mathbf{z}\in \mathbb{R}^n\)</span>, the
derivative <span class="math inline">\(D\mathbf{f}(\mathbf{z})\)</span>
is a linear map <span class="math display">\[\begin{align*}
D\mathbf{f}(\mathbf{z}): \mathbb{R}^{n} \to \mathbb{R}^m,
\end{align*}\]</span> i.e., for all <span
class="math inline">\(\mathbf{v}\in \mathbb{R}^n\)</span>, we have <span
class="math inline">\(D\mathbf{f}(\mathbf{z}) (\mathbf{v}) \in
\mathbb{R}^m\)</span>. Linearity implies that the derivative can be
identified with the <strong>Jacobian matrix</strong> <span
class="math display">\[\begin{align*}
J = D\mathbf{f}(\mathbf{z}) \in \mathbb{R}^{m \times n},
\end{align*}\]</span> so that the evaluation <span
class="math inline">\(D\mathbf{f}(\mathbf{z}) (\mathbf{v})\)</span> can
be realized by the matrix-vector multiplication <span
class="math inline">\(D\mathbf{f}(\mathbf{z})_{m\times n} \mathbf{v}_{n
\times 1} \in \mathbb{R}^m\)</span> with column vectors <span
class="math inline">\(\mathbf{v}\in \mathbb{R}^n\)</span>.</p>
<p>The Jacobian <span class="math inline">\(J\)</span> is given by <span
class="math display">\[\begin{align*}
    J =
\left( \left( \frac{\partial f_i(\mathbf{z})}{\partial z_j}
\right)_{j=1}^m\right)_{i=1}^n
    = \begin{bmatrix} \nabla f_1(\mathbf{z})^T \\ \vdots \\ \nabla
f_m(\mathbf{z})^T \end{bmatrix}_{m \times n}
=
    \begin{bmatrix}
    \frac{\partial f_1}{\partial z_1} &amp; \dots &amp; \frac{\partial
f_1}{\partial z_n}
    \\
    \frac{\partial f_2}{\partial z_1} &amp; \dots &amp; \frac{\partial
f_2}{\partial z_n}
    \\
    \vdots
    \\
    \frac{\partial f_m}{\partial z_1} &amp; \dots &amp; \frac{\partial
f_m}{\partial z_n}
    \end{bmatrix}_{m \times n}
    (\mathbf{z})
    = D\mathbf{f}(\mathbf{z}).
\end{align*}\]</span></p>
<p>More generally, for a differentiable function <span
class="math inline">\(\mathbf{f}: A \to B\)</span>, the derivative <span
class="math inline">\(D\mathbf{f}(\mathbf{a})\)</span> is a linear map
between <span class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> for all <span
class="math inline">\(\mathbf{a}\in A\)</span>.</p>
<p>For example, if <span class="math inline">\(\mathbf{f}: \mathbb{R}^n
\to \mathbb{R}^m\)</span> we have <span
class="math display">\[\begin{align*}
D\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^{m \times n}
\end{align*}\]</span> and therefore for any <span
class="math inline">\(\mathbf{z}\in \mathbb{R}^n\)</span>, the second
derivative <span class="math inline">\(D (D\mathbf{f})(\mathbf{z}) =:
D^2 \mathbf{f}(\mathbf{z})\)</span> is a linear map <span
class="math display">\[\begin{align*}
    D^2 \mathbf{f}(\mathbf{z}): \mathbb{R}^{m \times n} \to \mathbb{R}^n
\end{align*}\]</span><br />
which can be represented by a bilinear Hessian tensor, the elements of
which can be computed by evaluating it on the tensor basis <span
class="math inline">\(\mathbf{e}_i \otimes \mathbf{e}_j \in
\mathbb{R}^{m \times n}\)</span></p>
<p>A special case when <span class="math inline">\(\mathbf{f}:
\mathbb{R}^n \to \mathbb{R}^m\)</span> is linear, there exists <span
class="math inline">\(A \in \mathbb{R}^{m\times n}\)</span> such that
<span class="math inline">\(\mathbf{f}(\mathbf{z}) =
A\mathbf{z}\)</span>. In this case, <span
class="math inline">\(D\mathbf{f}(\mathbf{z}) = A\)</span> for all <span
class="math inline">\(\mathbf{z}\in \mathbb{R}^m\)</span> and <span
class="math inline">\(D\mathbf{f}(\mathbf{z}) \mathbf{v}=
A\mathbf{v}\)</span> for all <span class="math inline">\(\mathbf{v}\in
\mathbb{R}^n\)</span>.</p>
<h2 id="chain-rule">Chain rule</h2>
<p>Let <span class="math inline">\(\mathbf{f}: \mathbb{R}^n \to
\mathbb{R}^p\)</span> and <span class="math inline">\(\mathbf{h}:
\mathbb{R}^p \to \mathbb{R}^m\)</span>. Define <span
class="math inline">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^m\)</span>
as <span class="math inline">\(\mathbf{F}(\mathbf{x}) =
\mathbf{h}(\mathbf{f}(\mathbf{x}))\)</span>. Then, chain rule implies
<span class="math display">\[\begin{align*}
    [D\mathbf{F}(\mathbf{x})]_{m \times n} =
    [D\mathbf{h}(\mathbf{z})|_{\mathbf{z}= \mathbf{f}(\mathbf{x})}  ]_{m
\times p} [D\mathbf{f}(\mathbf{x})]_{p \times n}
\end{align*}\]</span><br />
Defining <span class="math inline">\(\mathbf{v}\)</span> as the
<strong>grad_output</strong> (a common name in the pytorch literature)
<span class="math display">\[\begin{align*}
    \mathbf{v}= D\mathbf{f}(\mathbf{x}).
\end{align*}\]</span><br />
we can write <span class="math display">\[\begin{align*}
    D\mathbf{F}(\mathbf{x}) = D\mathbf{h}(\mathbf{f}(\mathbf{x}))
(\mathbf{v}).
\end{align*}\]</span></p>
<p>In other words, <span class="math display">\[\begin{align*}
    J_{\mathbf{h}\circ \mathbf{f}} (\mathbf{x}) = J_{\mathbf{h}}
(\mathbf{f}(\mathbf{x})) J_{\mathbf{f}}(\mathbf{x}).
\end{align*}\]</span></p>
<h1 id="writing-custom-autograd-functions">Writing Custom
<code>autograd</code> functions</h1>
<p>When the loss function (or the model) involves composition with
functions that are not simple, we need to specify the derivative
manually. In fact, we do not need to specify the entire derivative
tensor, just the tensor-vector product rule.</p>
<p><a
href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">Tutorial</a>
gives some good examples.</p>
<p>In the <code>.backward()</code> method of a custom autograd function,
given a vector <span class="math inline">\(\mathbf{v}\in
\mathbb{R}^m\)</span>, we need to define the formula of <span
class="math inline">\(J\mathbf{v}\)</span>. This is known as the
<strong>Jacobian-vector product</strong>. The vector <span
class="math inline">\(\mathbf{v}\)</span> is usually referred to as
<strong>grad_output(s)</strong>.</p>
<p>The <span class="math inline">\(i\)</span>-th element of <span
class="math inline">\(J\mathbf{v}\)</span> (for <span
class="math inline">\(i=1, \dots, n\)</span>) is <span
class="math display">\[\begin{align*}
    (J \mathbf{v})_i = \sum_{k=1}^{m} \frac{\partial f_i}{\partial z_k}
v_k.
\end{align*}\]</span></p>
<h3 id="example-forward-difference-operator">Example: forward difference
operator</h3>
<p>The forward difference operator (with Dirichlet or periodic boundary)
<span class="math inline">\(\Delta^h : \mathbb{R}^n \to
\mathbb{R}^n\)</span> is given by <span
class="math display">\[\begin{align*}
    \Delta^h_i (\mathbf{z}) = \frac{1}{h} \left(  z_{i+1} -
z_{i}\right).
\end{align*}\]</span><br />
Then, the partial derivative is <span
class="math display">\[\begin{align*}
    \frac{\partial \Delta^h_i}{\partial z_k} = \frac{1}{h} \left(
\delta_{i+1,k} - \delta_{i,k} \right)
\end{align*}\]</span><br />
where <span class="math inline">\(\delta_{kj}\)</span> is Kronecker’s
delta. Therefore, for any <span class="math inline">\(\mathbf{v}\in
\mathbb{R}^n\)</span> we have <span
class="math display">\[\begin{align*}
    (D_\mathbf{z}\Delta^h(\mathbf{z}) \mathbf{v})_i =
\frac{1}{h}\sum_{k=1}^{n} (\delta_{i+1, k} - \delta_{i,k})v_k =
\frac{1}{h} (v_{i+1} - v_i).
\end{align*}\]</span></p>
<h3 id="example-legedre-polynomial">Example: Legedre polynomial</h3>
<p>If we are using an exotic function <span
class="math inline">\(g\)</span> for which <code>pytorch</code> does not
have the formula for derivate (and we do), we can define it
ourselves</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LegendrePolynomial3(torch.autograd.Function):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">    We can implement our own custom autograd Functions by subclassing</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">    torch.autograd.Function and implementing the forward and backward passes</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">    which operate on Tensors.</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(ctx, <span class="bu">input</span>):</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co">        In the forward pass we receive a Tensor containing the input and return</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co">        a Tensor containing the output. ctx is a context object that can be used</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co">        to stash information for backward computation. You can cache arbitrary</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co">        objects for use in the backward pass using the ctx.save_for_backward method.</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>        ctx.save_for_backward(<span class="bu">input</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> (<span class="dv">5</span> <span class="op">*</span> <span class="bu">input</span> <span class="op">**</span> <span class="dv">3</span> <span class="op">-</span> <span class="dv">3</span> <span class="op">*</span> <span class="bu">input</span>)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backward(ctx, grad_output):</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="co">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="co">        with respect to the output, and we need to compute the gradient of the loss</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="co">        with respect to the input.</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>        <span class="bu">input</span>, <span class="op">=</span> ctx.saved_tensors</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> grad_output <span class="op">*</span> <span class="fl">1.5</span> <span class="op">*</span> (<span class="dv">5</span> <span class="op">*</span> <span class="bu">input</span> <span class="op">**</span> <span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span>)</span></code></pre></div>
<p>We can <code>apply</code> this function within the loop and define a
loss function like this:</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># To apply our Function, we use Function.apply method. We alias this as &#39;P3&#39;.</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>P3 <span class="op">=</span> LegendrePolynomial3.<span class="bu">apply</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward pass: compute predicted y using operations; we compute</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># P3 using our custom autograd operation.</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> a <span class="op">+</span> b <span class="op">*</span> P3(c <span class="op">+</span> d <span class="op">*</span> x)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute and print loss</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> (y_pred <span class="op">-</span> y).<span class="bu">pow</span>(<span class="dv">2</span>).<span class="bu">sum</span>()</span></code></pre></div>
<h3 id="example-derivative-operator">Example: derivative operator</h3>
<p><a
href="https://stackoverflow.com/questions/58839721/how-to-define-a-loss-function-in-pytorch-with-dependency-to-partial-derivatives">This
discussion</a> demonstrates a nice example of solving an ODE with
initial data.</p>
<p>Computing numerical derivate of data with respect to another data can
be done using <code>torch.autograd.grad()</code>.</p>
<p>Given <span class="math inline">\(\mathbf{f}: \mathbb{R}^n \to
\mathbb{R}^m\)</span>, we have <span
class="math inline">\(D\mathbf{f}(\mathbf{z}): \mathbb{R}^n \to
\mathbb{R}^m\)</span> for all <span class="math inline">\(\mathbf{z}\in
\mathbb{R}^n\)</span>.</p>
<ul>
<li>If <span class="math inline">\(m=1\)</span>, we can use
<code>autograd.grad(output=f, input=z)</code> to compute <span
class="math inline">\(D\mathbf{f}(\mathbf{z})\)</span></li>
<li>If <span class="math inline">\(m \ge 1\)</span>, we can use
<code>autograd.grad(output=f, input=z, grad_output=w)</code> to compute
<span class="math inline">\(D\mathbf{f}(\mathbf{z})^T
\boldsymbol{\omega}\)</span>. For dimensional consistency, we need to
have <span class="math inline">\(\boldsymbol{\omega}\in
\mathbb{R}^m\)</span> so that the output of <code>autograd.grad()</code>
is in <span class="math inline">\(\mathbb{R}^n\)</span>.</li>
</ul>
<p>The <a
href="https://pytorch.org/docs/stable/generated/torch.autograd.grad.html">manual</a>
says</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>torch.autograd.grad(outputs, inputs, grad_outputs<span class="op">=</span><span class="va">None</span>, retain_graph<span class="op">=</span><span class="va">None</span>, create_graph<span class="op">=</span><span class="va">False</span>, only_inputs<span class="op">=</span><span class="va">True</span>, allow_unused<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
<p>Parameters</p>
<ul>
<li><p><code>outputs</code>: The output tensors with respect to which
gradients will be calculated. It has to be scalar valued, unless
<code>grad_outputs</code> is specified, in which case, the returned
value will be a vector-Jacobian product.</p></li>
<li><p><code>inputs</code>: The input tensors for which gradients are
being computed. These must be part of the computational graph.</p></li>
<li><p><code>grad_outputs</code>: This parameter is an optional external
gradient to be applied on <code>outputs</code>. This is the vector <span
class="math inline">\(\mathbf{v}\)</span> in the vector-Jacobian product
<span class="math inline">\(\mathbf{v}^TJ\)</span>. <code>None</code>
values can be specified for scalar Tensors or ones that don’t require
grad. If a <code>None</code> value would be acceptable for all
grad_tensors, then this argument is optional. Default:
<code>None</code>.</p></li>
<li><p><code>retain_graph</code>: When set to <code>True</code>, the
computation graph used to compute the gradients will be retained,
allowing for further operations.</p></li>
<li><p><code>create_graph</code>: If <code>True</code>, a new
computational graph is created, enabling higher-order derivatives to be
computed.</p></li>
<li><p><code>allow_unused</code>: If <code>True</code>, it returns
<code>None</code> for input tensors unused in the computation;
otherwise, it throws an error.</p></li>
</ul>
<h3 id="example-element-wise-square">Example: element-wise square</h3>
<p>Consider the <em>element-wise square</em> function <span
class="math inline">\(\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^n\)</span>
given by <span class="math inline">\(f_i(\mathbf{x}) = x_i^2\)</span>
for all <span class="math inline">\(i=1, \dots, n\)</span>. Then, <span
class="math display">\[\begin{align*}
\frac{\partial f_i}{\partial x_k} = 2x_k \delta_{ik}.
\end{align*}\]</span> The Jacobian is $J = $ Diag<span
class="math inline">\((2x_1, \dots, 2x_n)\)</span>, a symmetric
matrix.</p>
<p>To obtain the <em>element-wise double</em> operator <span
class="math inline">\(\mathbf{h}: \mathbb{R}^n \to \mathbb{R}^n\)</span>
given by <span class="math inline">\(h_i(\mathbf{x}) = 2 x_i\)</span> as
a derivative of <span class="math inline">\(\mathbf{f}\)</span>, we can
either compute <span class="math inline">\(J^T\mathbf{v}\)</span> where
<span class="math inline">\(\mathbf{v}= \mathcal{1} \in
\mathbb{R}^n\)</span>, a vector of ones (<span
class="math inline">\(\mathcal{1}_i =1\)</span> for all <span
class="math inline">\(i=1, \dots, n\)</span>).</p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">5</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>ones <span class="op">=</span> torch.ones_like(x)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> x <span class="op">**</span><span class="dv">2</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the derivatives</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>grads <span class="op">=</span> torch.autograd.grad(outputs<span class="op">=</span>z, inputs<span class="op">=</span>x, grad_outputs<span class="op">=</span>ones)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;x&#39;</span>, x)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;ones&#39;</span>, ones)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;grads&#39;</span>, grads)</span></code></pre></div>
<p>which returns</p>
<pre><code>x tensor([-10.,  -5.,   0.,   5.,  10.], requires_grad=True)
ones tensor([1., 1., 1., 1., 1.])
grads (tensor([-20., -10.,   0.,  10.,  20.]),)</code></pre>
<p>The input data be a higher-order tensor. In that case, the
<code>grad_output</code> has to be the same shape as <code>output</code>
to return <span class="math inline">\(D\mathbf{f}(\mathbf{z})^T
\mathbf{v}\)</span> where <span
class="math inline">\(\mathbf{f}\)</span>=<code>output</code>, <span
class="math inline">\(\mathbf{z}\)</span>=<code>input</code>, and <span
class="math inline">\(\mathbf{v}\)</span>=<code>grad_output</code>.</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">5</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>).<span class="bu">pow</span>(torch.tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]))</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> torch.tensor([[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">1.</span>]])</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> torch.matmul(A, x)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the derivatives</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>ones <span class="op">=</span> torch.ones_like(z)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>grads <span class="op">=</span> torch.autograd.grad(outputs<span class="op">=</span>z, inputs<span class="op">=</span>x, grad_outputs<span class="op">=</span>ones)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;x&#39;</span>, x)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;ones&#39;</span>, ones)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;grads&#39;</span>, grads)</span></code></pre></div>
<p>returns</p>
<pre><code>x tensor([[  -10.,   100., -1000.],
        [   -5.,    25.,  -125.],
        [    0.,     0.,     0.],
        [    5.,    25.,   125.],
        [   10.,   100.,  1000.]], grad_fn=&lt;PowBackward1&gt;)
z tensor([[  0., 250.,   0.],
        [  5., 225., 125.]], grad_fn=&lt;MmBackward0&gt;)
ones tensor([[1., 1., 1.],
        [1., 1., 1.]])
grads (tensor([[2., 2., 2.],
        [1., 1., 1.],
        [2., 2., 2.],
        [2., 2., 2.],
        [2., 2., 2.]]),)</code></pre>
<p>By enabling <code>create_graph=True</code>, one can compute the
higher-order derivates like this:</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.tensor(<span class="fl">1.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.tensor(<span class="fl">2.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> a <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> t </span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> a <span class="op">*</span> t <span class="op">+</span> t <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> b<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> c</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>grad_a, grad_t <span class="op">=</span> torch.autograd.grad(outputs<span class="op">=</span>d, inputs<span class="op">=</span>(a, t), create_graph<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Computing higher-order derivatives</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>second_order_grad_a <span class="op">=</span> torch.autograd.grad(grad_a, a)[<span class="dv">0</span>]</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>second_order_grad_t <span class="op">=</span> torch.autograd.grad(grad_t, t)[<span class="dv">0</span>]</span></code></pre></div>
<p>This way, we can compute the (partial) derivatives of a data
(<code>outputs</code>) with respect to grid points
(<code>inputs</code>). This is applicable to solving ODEs with
prescribed boundary and/or initial value.</p>
<hr />
<h1 id="tex-and-markdown-conversion-to-html-with-pandoc">Tex and
markdown conversion to html with pandoc</h1>
<p>Look at these <a
href="https://github.com/mathjax/MathJax-demos-web?tab=readme-ov-file">many
examples</a> for inspiration.</p>
<ul>
<li>To use latex goodies such as snippet completion etc in vim, set</li>
</ul>
<pre class="vim"><code>:setfiletype pandoc.tex</code></pre>
<ul>
<li>Put all latex preamble in the header part of the <code>.md</code>
file <a
href="https://pandoc.org/MANUAL.html#extension-yaml_metadata_block">source</a>
like this</li>
</ul>
<div class="sourceCode" id="cb19"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="pp">---</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span><span class="kw">:</span><span class="at"> Readme</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="fu">author</span><span class="kw">:</span><span class="at"> Author</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="fu">header-includes</span><span class="kw">: </span><span class="ch">|</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    \usepackage{amsmath, amssymb, amsthm, amsfonts, color, bm}</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    \newcommand{\R}{\mathcal{R}}</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    \newcommand{\ww}{\boldsymbol{\omega}}</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    \newcommand{\xx}{\mathbf{x}}</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="pp">---</span></span></code></pre></div>
<p>Converting a tex file into html
<code>pandoc file.tex -o file.html</code> uses unicode by default to
render math symbols. We need to use <code>mathjax</code> for a nicer
rendering. Other options are</p>
<pre><code>--mathml, --webtex, --mathjax, --katex</code></pre>
<p>and demos can be found in pandoc <a
href="https://pandoc.org/demos.html">demos</a>.</p>
<p>According to <a
href="https://pandoc.org/chunkedhtml-demo/3.6-math-rendering-in-html.html">pandoc
documentation</a> One need to specify the url of the <code>.js</code>
file that would be used to convert math into mathjax. By default pandoc
uses some link form some content delivery network (CDN), which does not
work on firefox at the first attempt. So we can specify the url like
this:</p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pandoc</span> math.text <span class="at">-s</span> <span class="at">--mathjax</span><span class="op">=</span>https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js <span class="at">-o</span>   mathMathJax.html</span></code></pre></div>
<p>There are other CDN locations on <a
href="https://docs.mathjax.org/en/latest/web/start.html#cdn-list">mathjax
documentation</a> from sites like</p>
<ul>
<li>jsdelivr.com [latest or specific version] (recommended)</li>
<li>unpkg.com [latest or specific version]</li>
<li>cdnjs.com</li>
<li>raw.githack.com</li>
<li>gitcdn.xyz</li>
<li>cdn.statically.io</li>
</ul>
<div class="sourceCode" id="cb22"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pandoc</span> README.md <span class="at">-s</span> <span class="at">--mathjax</span><span class="op">=</span>https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js <span class="at">-o</span>   README.html</span></code></pre></div>
<h2 id="latex-writing-guide-for-mathjax">LaTeX writing guide for
MathJax</h2>
<p>We want our <code>.md</code> file to convert to pdf or tex without
much hassle. While using mathjax, a few things to remember for a quick
conversion.</p>
<ul>
<li>Typically, in the header of any website using latex code, we need to
add this script to load MathJax</li>
</ul>
<div class="sourceCode" id="cb23"><pre
class="sourceCode xml"><code class="sourceCode xml"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>&lt;<span class="kw">script</span><span class="ot"> type=</span><span class="st">&quot;text/javascript&quot;</span><span class="ot"> id=</span><span class="st">&quot;MathJax-script&quot;</span><span class="ot"> async</span> <span class="er">src</span><span class="ot">=</span><span class="st">&quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js&quot;</span>&gt; &lt;/<span class="kw">script</span>&gt;</span></code></pre></div>
<p>Here <code>tex-chtml-full</code> is a collection of components
(combination of various packages and output font formats).</p>
<ul>
<li>You can load any out of the following components <a
href="https://docs.mathjax.org/en/latest/web/components/index.html">link</a>
<ul>
<li>tex-chtml</li>
<li>tex-chtml-full</li>
<li>tex-svg</li>
<li>tex-svg-full</li>
<li>tex-mml-chtml</li>
<li>tex-mml-svg</li>
<li>mml-chtml</li>
<li>mml-svg</li>
</ul></li>
<li>Various properties of the parser can be changed by adding a config
script <strong>before</strong> the <code>.js</code> file is called
within the <code>&lt;script&gt;</code> tag of the header of the
<code>.md</code> file</li>
</ul>
<div class="sourceCode" id="cb24"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="at">&lt;script&gt;</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="at">MathJax = {</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">tex</span><span class="kw">:</span><span class="at"> </span><span class="kw">{</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">inlineMath</span><span class="kw">:</span><span class="at"> </span><span class="kw">[[</span><span class="st">&#39;$&#39;</span><span class="kw">,</span><span class="at"> </span><span class="st">&#39;$&#39;</span><span class="kw">],</span><span class="at"> </span><span class="kw">[</span><span class="st">&#39;\\(&#39;</span><span class="kw">,</span><span class="at"> </span><span class="st">&#39;\\)&#39;</span><span class="kw">]],</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">tags</span><span class="kw">:</span><span class="at"> </span><span class="st">&#39;ams&#39;</span><span class="kw">,</span><span class="at">              // equation numbering </span><span class="st">&#39;ams&#39;</span><span class="at"> or </span><span class="st">&#39;all&#39;</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">}</span><span class="at">,</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">svg</span><span class="kw">:</span><span class="at"> </span><span class="kw">{</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">fontCache</span><span class="kw">:</span><span class="at"> </span><span class="st">&#39;global&#39;</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">}</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="at">};</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="at">&lt;/script&gt;</span></span></code></pre></div>
<ul>
<li>Equation numbering is turned off by default, it can be enabled using
the config</li>
</ul>
<div class="sourceCode" id="cb25"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tex</span><span class="kw">:</span><span class="at"> </span><span class="kw">{</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">tags</span><span class="kw">:</span><span class="at"> </span><span class="st">&#39;ams&#39;</span><span class="kw">,</span><span class="at">              // equation numbering </span><span class="st">&#39;ams&#39;</span><span class="at"> or </span><span class="st">&#39;all&#39;</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="kw">}</span></span></code></pre></div>
<ul>
<li>Tex environments <strong>not</strong> enclosed by double
<code>$$</code>s will be parsed by default as latex code. The default
option</li>
</ul>
<pre><code>    processEnvironments: true, // process \begin{xxx}...\end{xxx} outside math mode</code></pre>
<p>is responsible for this.</p>
<p>For example,</p>
<pre><code>```
\begin{align*}
x^2
\end{align*}
```</code></pre>
<p>is fine.</p>
<p>This is actually desired when converting <code>.md</code> to
<code>.tex</code>. Otherwise, math environments enclosed by double
<code>$$</code>s will throw error in the tex file. It needs to be remove
manually before or after conversion before compiling latex. But
developing the habit of writing math environments without the
<code>$$</code>s causes some pain since <code>vim-pandoc</code> does not
render the math symbols within the environments in vim.</p>
<ul>
<li><p>If you are using <code>\usepackage{bm}</code> for bold fonts and
using commands like <code>\boldsymbol{\sigma}</code> etc (apparently
better alternative to <code>\pmb{}</code>), make sure to put braces
around it when using as subscript or superscript. For example, use
<code>f_{\boldsymbol{\sigma}}(x)</code> instead of
<code>f_\boldsymbol{\sigma}(x)</code>. The second usage will put
<code>(x)</code> also within the subscript when you convert it into tex.
This is a very weird behavior since you would think both are Tex
commands.</p></li>
<li><p>Bullet points: make sure to leave an empty line before the first
top level bullet point. For sub-bullet points, no need to have empty
line. Empty lines between bullet points makes the bullets more
sparse.</p></li>
</ul>
<h2 id="diagrams">Diagrams</h2>
<p>My observation is that using a freehand drawing tool like inkscape
takes less time and provides more flexibility for creating diagrams.
Markdown-friendly tools like <code>mermaid</code> required additional
set up and does not seem to support latex within diagram. Maybe once can
use latex <code>tikz</code> diagrams within markdown, but the whole
point was to move away from programmable diagrams. Still:</p>
<p>Install pandoc filter for mermaid from <a
href="https://github.com/raghur/mermaid-filter?tab=readme-ov-file">git</a>
using</p>
<div class="sourceCode" id="cb28"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="ex">npm</span> install <span class="at">--global</span> mermaid-filter</span></code></pre></div>
<p>Use it with <code>-F</code> in pandoc</p>
<pre><code>pandoc  -F mermaid-filter something.md -o something.html </code></pre>
<p>A sample block of code</p>
<pre class="pandoc"><code>```{.mermaid format=png scale=5 caption=&#39;Computational graph&#39;}
%%{init: {&#39;theme&#39;:&#39;neutral&#39;}}%%
  graph TD
    a --&gt; b</code></pre>
<p>```</p>
<p>produces a flowchart-like diagram.</p>
<h2 id="marp-and-usual-markdown">Marp and usual markdown</h2>
<p>Converting marp-focused <code>.md</code> file into html using
<code>pandoc</code> is very glitchy. Few things that do not work so
far</p>
<ul>
<li>Putting latex preambles to the yaml header</li>
<li>Custom css for creating columns: math within the custom css does not
render. Pandoc is not able to accept the marp theme css</li>
<li>Bullet points render in a single line</li>
<li></li>
</ul>
<h2 id="github-readme-compatibility">Github readme compatibility</h2>
<p>The markdown files with header does not render in github webpages,
even though both are mathjax.</p>
<h2 id="table-tests">Table tests</h2>
<ul>
<li>So far, there seems to be 4 types of tables on markdown <a
href="https://pandoc.org/chunkedhtml-demo/8.9-tables.html">see</a>. Here
are some examples. Using a custom css, I can enhance the table
displays</li>
</ul>
<table style="width:86%;">
<caption>Here’s a multiline table without a header.</caption>
<colgroup>
<col style="width: 16%" />
<col style="width: 11%" />
<col style="width: 22%" />
<col style="width: 36%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;">First</td>
<td style="text-align: left;">row</td>
<td style="text-align: right;">12.0</td>
<td style="text-align: left;">Example of a row that spans multiple
lines.</td>
</tr>
<tr class="even">
<td style="text-align: center;">Second</td>
<td style="text-align: left;">row</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: left;">Here’s another one. Note the blank line
between rows.</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Third</td>
<td style="text-align: left;">row</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: left;">Here’s another one. Note the blank line
between rows.</td>
</tr>
<tr class="even">
<td style="text-align: center;">Fourth</td>
<td style="text-align: left;">row</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: left;">Here’s another one. Note the blank line
between rows.</td>
</tr>
</tbody>
</table>
<table style="width:86%;">
<caption>More table with header</caption>
<colgroup>
<col style="width: 16%" />
<col style="width: 11%" />
<col style="width: 22%" />
<col style="width: 36%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">H1</th>
<th style="text-align: center;">H2</th>
<th style="text-align: center;">H3</th>
<th style="text-align: center;">H4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">First</td>
<td style="text-align: center;">row</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">Example of a row that spans multiple
lines.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Second</td>
<td style="text-align: center;">row</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">Here’s another one. Note the blank line
between rows.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Third</td>
<td style="text-align: center;">row</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">Here’s another one. Note the blank line
between rows.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Fourth</td>
<td style="text-align: center;">row</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">Here’s another one. Note the blank line
between rows.</td>
</tr>
</tbody>
</table>
<table style="width:64%;">
<caption>Here’s a grid table</caption>
<colgroup>
<col style="width: 30%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
</colgroup>
<thead>
<tr class="header">
<th rowspan="2">Location</th>
<th colspan="3">Temperature 1961-1990 in degree Celsius</th>
</tr>
<tr class="odd">
<th>min</th>
<th>mean</th>
<th>max</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Antarctica</td>
<td>-89.2</td>
<td>N/A</td>
<td>19.8</td>
</tr>
<tr class="even">
<td>Earth</td>
<td>-89.2</td>
<td>14</td>
<td>56.7</td>
</tr>
</tbody>
</table>
<h2 id="referencing">Referencing</h2>
</body>
</html>
