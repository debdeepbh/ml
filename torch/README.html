<!doctype html>
<html >
<head>
    
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <!--[if lt IE 9]>
                <script src="http://css3-mediaqueries-js.googlecode.com/svn/trunk/css3-mediaqueries.js"></script>
        <![endif]-->
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta http-equiv="Content-Style-Type" content="text/css" />

    <!-- <link rel="stylesheet" type="text/css" href="template.css" /> -->
    <link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/template.css" />

    <link href="https://vjs.zencdn.net/5.4.4/video-js.css" rel="stylesheet" />

    <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
    <!-- <script type='text/javascript' src='menu/js/jquery.cookie.js'></script> -->
    <!-- <script type='text/javascript' src='menu/js/jquery.hoverIntent.minified.js'></script> -->
    <!-- <script type='text/javascript' src='menu/js/jquery.dcjqaccordion.2.7.min.js'></script> -->

    <!-- <link href="menu/css/skins/blue.css" rel="stylesheet" type="text/css" /> -->
    <!-- <link href="menu/css/skins/graphite.css" rel="stylesheet" type="text/css" /> -->
    <!-- <link href="menu/css/skins/grey.css" rel="stylesheet" type="text/css" /> -->
  
    <!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
        
  
    <!-- <script src="script.js"></script> -->
  
    <!-- <script src="jquery.sticky-kit.js "></script> -->
    <script type='text/javascript' src='https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/js/jquery.cookie.js'></script>
    <script type='text/javascript' src='https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/js/jquery.hoverIntent.minified.js'></script>
    <script type='text/javascript' src='https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/js/jquery.dcjqaccordion.2.7.min.js'></script>

    <link href="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/css/skins/blue.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/css/skins/graphite.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/css/skins/grey.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.rawgit.com/ryangrose/easy-pandoc-templates/948e28e5/css/elegant_bootstrap.css" rel="stylesheet" type="text/css" />
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
    <script src="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/script.js"></script>
  
    <script src="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/jquery.sticky-kit.js"></script>
    <meta name="generator" content="pandoc" />
  <title>README</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
  </style>
</head>
<body>

    
    <div class="container">
    <div class="row">
            <div class="span12">
            <h1 id="pytorch-basics"><span class="header-section-number">1</span> Pytorch basics</h1>
<p><a href="https://d2l.ai/chapter_preliminaries/ndarray.html">book</a></p>
<h1 id="install"><span class="header-section-number">2</span> Install</h1>
<pre><code>pip3 install torch</code></pre>
<h1 id="topics"><span class="header-section-number">3</span> Topics</h1>
<h2 id="readwrite-with-pandas-rw.py"><span class="header-section-number">3.1</span> Read/write with pandas <a href="rw.py"><code>rw.py</code></a></h2>
<ul>
<li>replacing missing values with the mean of the column <code>.fillna(value=value, inplace=True)</code></li>
<li>converting categorical variables (e.g. string) to indicator variables (one-hot) <code>pd.get_dummies(data, dummy_na=True)</code></li>
<li>extracting pandas table data segments as tensor etc <code>data.iloc[:, list].values</code></li>
</ul>
<h2 id="tensor-handling-manipulation.py"><span class="header-section-number">3.2</span> Tensor handling <a href="manipulation.py"><code>manipulation.py</code></a></h2>
<ul>
<li>reshaping <code>reshape</code></li>
<li>concatenating along axis <code>.cat((x,y), dim=1)</code></li>
<li>zero tensor of size matching another tensor <code>torch.zeros_like(another_tensor)</code></li>
<li>in-place addition <code>q[:] = x + y</code> as opposed to <code>q = x+y</code></li>
</ul>
<h2 id="linear-algebra-linalg.py"><span class="header-section-number">3.3</span> Linear algebra <a href="linalg.py"><code>linalg.py</code></a></h2>
<ul>
<li>matrix-matrix product with <code>torch.mm(A, B)</code></li>
<li>matrix-vector product with <code>torch.mv(A, v)</code></li>
<li>matrix product of tensors with <code>torch.matmul(A, v)</code> or <code>torch.matmul(A,B)</code> (more general)</li>
<li>operations over dimension <code>dim=</code> and keeping dimension <code>keepdims=True</code></li>
</ul>
<h2 id="auto-differentiation-autodiff.py"><span class="header-section-number">3.4</span> Auto differentiation <a href="autodiff.py"><code>autodiff.py</code></a></h2>
<ul>
<li>define a tensor as a variable of differentiation (or gradient) with <code>x.requires_grad_(True)</code> (in addition to as a point of evaluation)</li>
<li>populate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> with the gradient of a functional <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">y = f(x)</annotation></semantics></math> w.r.t. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> evaluated at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">x=x</annotation></semantics></math> with <code>y.backward()</code> and retrieve it with <code>x.grad()</code></li>
<li>clean up the gradient value before computing a new gradient with <code>x.grad.zero_()</code></li>
<li>treat a function <code>y</code> of a <code>requires_grad_()</code> variable as constant with <code>y.detach()</code></li>
</ul>
<h2 id="linear-regression-from-scratch-linreg.py"><span class="header-section-number">3.5</span> Linear regression from scratch <a href="linreg.py"><code>linreg.py</code></a></h2>
<ul>
<li><strong>Goal:</strong> estimate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>,</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">m, b</annotation></semantics></math> to fit data <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>m</mi><mi>X</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = mX + b</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> is feature and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> is label</li>
<li>create synthetic data using <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>m</mi><mi>X</mi><mo>+</mo><mi>b</mi><mo>+</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">y = mX + b + n</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> is normal(0,1) <code>torch.normal</code></li>
<li>a batch iterator shuffles indices of the data and returns (<code>yield</code>s) batches of data so that every call of the iterator returns a non-overlapping subset. Using <code>yield</code> instead of <code>return</code> makes a function behave like a <code>for</code> loop by remembering all the previous calls.</li>
<li><p><strong>Prediction model:</strong> <code>y_hat = m_est * X + b_est</code> and <strong>loss function</strong> to minimize is <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><munder><mo>∑</mo><mrow><mi>m</mi><mi>i</mi><mi>n</mi><mi>i</mi><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></munder><mo stretchy="false" form="prefix">|</mo><msub><mi>y</mi><mrow><mi>h</mi><mi>a</mi><mi>t</mi></mrow></msub><mo>−</mo><mi>y</mi><msup><mo stretchy="false" form="prefix">|</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">
loss =  \sum_{minibatch} | y_{hat} - y|^2
</annotation></semantics></math></p></li>
<li><p><strong>Minimization using stochastic gradient descent:</strong></p></li>
</ul>
<p>Initial guess: <span class="math display">$$
m = normal(0, 0.01)
\\
b = zeros
$$</span> Iteration: for each <code>epoch</code> (iteration of minimization step) and for each minibatch,</p>
<ol type="1">
<li>compute the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>n</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>m</mi><mi>i</mi><mi>n</mi><mi>i</mi><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">loss on current minibatch</annotation></semantics></math></li>
<li>compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>m</mi><mo>,</mo><mi>b</mi><mo stretchy="false" form="postfix">)</mo></mrow></msub><mo stretchy="false" form="prefix">(</mo><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>n</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>m</mi><mi>i</mi><mi>n</mi><mi>i</mi><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\nabla_{(m,b)} (loss on current minibatch)</annotation></semantics></math> using backward differentiation</li>
<li><span class="math display">$$
(m,b)_{new} = (m,b)_{old} - \Delta t \nabla_{(m,b)} (loss on current minibatch) / batchsize
\\
(m,b)_{new}.grad.zero_()
$$</span> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Δ</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">\Delta t</annotation></semantics></math> is the learning rate, gradient is computed using auto differentiation with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>,</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">m, b</annotation></semantics></math> as <code>requires_grad_(True)</code> with initialization of normal(0,0.01) and <code>zeros</code>, respectively.</li>
</ol>
<ul>
<li>The stochastisticity comes from the randomness associated with picking the subsets (minibatches).</li>
<li><p>The loss at each epoch is computed to be the loss for the last minibatch. Probabilistically, this would be the smallest loss among all other minibatches.</p></li>
<li><p>At each epoch, the estimate for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>m</mi><mo>,</mo><mi>b</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(m,b)</annotation></semantics></math> is improved (learned) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mi>/</mi><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">N/batchsize</annotation></semantics></math> number of times, based on the estimate from the previous minibatch.</p></li>
</ul>
<h2 id="linear-regression-ready-made-linalg_ready.py"><span class="header-section-number">3.6</span> Linear regression ready-made <a href="linalg_ready.py"><code>linalg_ready.py</code></a></h2>
<h3 id="iterator"><span class="header-section-number">3.6.1</span> Iterator</h3>
<p>Data iterator library comes from</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1"><span class="im">from</span> torch.utils <span class="im">import</span> data</a></code></pre></div>
<p>It provides an iterator that loads inside an iterator function</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" title="1"><span class="kw">def</span> load_array(data_arrays, batch_size):  </a>
<a class="sourceLine" id="cb3-2" title="2">    dataset <span class="op">=</span> data.TensorDataset(<span class="op">*</span>data_arrays)</a>
<a class="sourceLine" id="cb3-3" title="3">    <span class="cf">return</span> data.DataLoader(dataset, batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</a></code></pre></div>
<p>and called using</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" title="1">data_iter <span class="op">=</span> load_array((features, labels), batch_size)</a></code></pre></div>
<h3 id="neural-network-structure"><span class="header-section-number">3.6.2</span> Neural network structure</h3>
<ul>
<li>A single layer Sequential neural network with x-dim = 2 (feature) and y-dim =1 (label) can be created using</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" title="1">net <span class="op">=</span> nn.Sequential(nn.Linear(<span class="dv">2</span>,<span class="dv">1</span>))</a></code></pre></div>
<ul>
<li>Initialize the weights and the bias of the first layer</li>
</ul>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" title="1">net[<span class="dv">0</span>].weight.data.normal_(<span class="dv">0</span>, <span class="fl">0.01</span>)</a>
<a class="sourceLine" id="cb6-2" title="2">net[<span class="dv">0</span>].bias.data.fill_(<span class="dv">0</span>)</a></code></pre></div>
<p>Here, <code>net[0].weight</code> is <code>m</code> and <code>net[0].bias</code> is <code>b</code>.</p>
<ul>
<li>Define the loss function as mean-squared error</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" title="1">loss <span class="op">=</span> nn.MSELoss()</a></code></pre></div>
<ul>
<li>Train using</li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" title="1"><span class="cf">for</span> epoch <span class="kw">in</span> <span class="dv">3</span>:</a>
<a class="sourceLine" id="cb8-2" title="2">    <span class="cf">for</span> X, y <span class="kw">in</span> data_iter:</a>
<a class="sourceLine" id="cb8-3" title="3">        <span class="co"># l obtained through nn.MSELoss() already has the sum (then avg) of the loss over the minibatch</span></a>
<a class="sourceLine" id="cb8-4" title="4">        l <span class="op">=</span> loss(net(X) ,y)</a>
<a class="sourceLine" id="cb8-5" title="5">        <span class="co"># same as setting zero_grad() to each parameter in net.parameters()</span></a>
<a class="sourceLine" id="cb8-6" title="6">        trainer.zero_grad()</a>
<a class="sourceLine" id="cb8-7" title="7">    <span class="co"># backward differentiation and store in net.parameters()</span></a>
<a class="sourceLine" id="cb8-8" title="8">        l.backward()</a>
<a class="sourceLine" id="cb8-9" title="9">        <span class="co"># step() will update the value of the loss-minimizer parameters</span></a>
<a class="sourceLine" id="cb8-10" title="10">        trainer.step()</a>
<a class="sourceLine" id="cb8-11" title="11">    l <span class="op">=</span> loss(net(features), labels)</a>
<a class="sourceLine" id="cb8-12" title="12">    <span class="bu">print</span>(<span class="ss">f&#39;epoch </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, loss </span><span class="sc">{l:f}</span><span class="ss">&#39;</span>)</a></code></pre></div>
<h3 id="reading-the-loss-minimizing-parameters"><span class="header-section-number">3.6.3</span> Reading the loss-minimizing parameters</h3>
<ul>
<li>Read the trained wights <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math> and bias <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math> using</li>
</ul>
<pre><code>w = net[0].weight.data
b = net[0].bias.data</code></pre>
<h2 id="downloading-and-viewing-the-fashion-datasets-fashion.py"><span class="header-section-number">3.7</span> Downloading and viewing the fashion datasets <a href="fashion.py"><code>fashion.py</code></a></h2>
<ul>
<li>Download the dataset from <code>torchvision.datasets.FashionMNIST()</code></li>
<li>show the image using <code>imshow</code></li>
</ul>
            </div>
    </div>
  </div>
  <script src="https://vjs.zencdn.net/5.4.4/video.js"></script>

</body>
</html>
